
--- 
title: 'TMA4300 Computer Intensive Statistical Methods Exercise 2, Spring 2021'
output:
  html_document:
    toc: yes
    toc_depth: 2
    toc_float: yes
  pdf_document:
    toc: no
    toc_depth: '2'
date: "`r format(Sys.time(), '%d.%m.%Y')`"
subtitle: 'Group members: Eide, Jonathan and Lima-Eriksen, Leik'
---

\usepackage{amsmath}

\newcommand{\indep}{\perp \!\!\! \perp}
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1} }}
\newcommand{\matr}[1]{\boldsymbol{\mathbf{#1} }}
\newcommand{\Var}{\textrm{Var}}
\newcommand{\E}{\textrm{E}}
\newcommand{\Cov}{\textrm{Cov}}
\newcommand{\Corr}{\textrm{Corr}}

# Problem A: Modelling coal-mining disasters timeseries

In this problem we will take a closer look at a famous data set of time intervals between successive coal-mining disasters
in the UK involving ten or more men killed (Jarrett, 1979). The data is for the period March 15th 1851 to
March 22nd 1962. It contains a list of floating-point numbers where the integer part specifies in which year the accident occurred, and the decimal part specifies at which part of the year it occurred.

```{R rpackages,eval=TRUE,echo=FALSE}
set.seed(1234)

# Load the coal-mining data set in the variable `coal`
# The first and last records in the variable is the
# start and end dates respectively
library(boot)

# List all other used library here in the preamble
library(ggplot2)
library(dplyr)
library(invgamma)
library(ggpubr)
```


In order to get an impression of how the data we will be modeling looks like, we start off by making a histogram with year along the $x$-axis and cumulative
number of disasters along the $y$-axis. The plot has been provided in the figure below:

```{R}
coal_by_year <- data.frame(year=as.integer(coal$date))  %>% count(year)
coal_by_year$cum_n <- cumsum(coal_by_year$n)
ggplot(data=coal_by_year, aes(x=year, y=cum_n, group=1)) +
  geom_line() +
  geom_point() +
  ggtitle("Cumulative number of accidents in coal-mines vs year")
``` 

Observe that between 1850 and 1890 the number of accidents per year seems to be quite constant, as the cumulative plot has a linear trend. Afterwards, we see that the trends flattens more out between 1900 and 1925. This should make sense, because advancements in the safety for the coal-miners were likely to happen at some point, which would drag this rate down. Another explanation could be a decrease in demand for coal, which would result in fewer accidents since fewer people were required to work in the coal-mines. It then increased between 1930 and 1947. This could either be because the ongoing wars required people to work under fewer safety constraints. Another reason could be that the wars increased the demand for coal, which meant that more people had to work in the mines. Both reasons would further explain why the rate fell after the wars.

We see that the dataset has several distinct periods where each period has an approximately constant rate at which the accidents occurred. It therefore seems reasonable to assume that the coal-mining disasters
follow an inhomogeneous Poisson process with intensity function $\lambda(t)$ (number of events per year), where we assume
$\lambda(t)$ to be piecewise constant with $n$ breakpoints. Let $t_0$ and $t_{n+1}$ to denote the start and end times for the
data set and let $t_k; k = 1, \dots, n$ denote the breakpoints of the intensity function. Thus,

$$
  \lambda(t) = \begin{cases}
    \lambda_{k-1} \quad &\forall \quad t \in [t_{k-1}, t_k), \quad k = 1, \dots, n \\
    \lambda_k  \quad &\forall \quad t \in [t_n, t_{n+1}].
  \end{cases}
$$

The parameters of this model is thereby $t_1, \dots, t_n$ and  $\lambda_0, \lambda_n$, where $t_0 < t_1 < \dots < t_n < t_{n+1}$. By
subdividing the observation period into short intervals, and taking the limit when the length of these intervals go to zero, one can derive the likelihood function for the observed data as

\begin{align*}
  \pi(\vect{x} |t_1, \dots, t_n, \lambda_0, \dots, \lambda_n) &= \exp\left( -\int_{t_0}^{t_{n+1}} \lambda(t) \text{d} t\right) \prod_{k=0}^n \lambda^{y_k} \\
  &= \exp\left( -\sum_{k=0}^n \lambda_k (t_{k+1} - t_k) \right) \prod_{k=0}^n \lambda^{y_k}
\end{align*}

where $\vect{x}$ is the observed data and $y_k$ is the number of observed disasters in the period $t_k$ to $t_{k+1}$. Assume
$t_1, \dots, t_n$ to be apriori uniformly distributed on the allowed values, and $\lambda_0, \dots, \lambda_n$ to be apriori independent
of $t_1, \dots, t_n$ and apriori independent of each other. Apriori we assume all $\lambda_0, \dots , \lambda_n$ to be distributed from
the same Gamma distribution with shape parameter $\alpha = 2$ and scale parameter $\beta$, i.e.

$$
  \pi(\lambda_i | \beta) = \frac{1}{\beta^2} \lambda_i \exp\left\{-\frac{\lambda_i}{\beta}\right\} \quad \forall \quad \lambda_i \geq 0
$$

Finally, for $\beta$ we use the improper prior

$$
  \pi(\beta) \propto \frac{\exp\left\{-\frac{1}{\beta}\right\}}{\beta} \quad \forall \quad \beta > 0
$$

Now assume $n = 1$. Then our model parameters are $\vect{\theta} = \begin{pmatrix}t_1 & \lambda_0 &\lambda_1 & \beta \end{pmatrix}^T$. The prior $\pi(\vect{\theta})$ can be found by using the fact that $t_1$, $\lambda_0$ and $\lambda_1$ are independent of each other:

\begin{align*}
  \pi(\vect{\theta}) &= \pi(t_1) \times f(\lambda_0 | \beta) \times \pi(\lambda_1 | \beta) \times \pi(\beta) \\
  &\propto \frac{1}{t_2 - t_0} \cdot \frac{1}{\beta^5} \lambda_0 \lambda_1 \exp\left\{-\frac{1 + \lambda_1 + \lambda_0}{\beta}\right\}
\end{align*}

Note that $t_0$ and $t_2$ are the start and end of the time series respectively, and so they are not parameters in our model. We are interested in finding the posterior distribution $\pi(\vect{\theta} | \vect{x})$ up to a proportionality constant. This can be found by using Bayes law:

\begin{align*}
  \pi(\vect{\theta} | \vect{x}) &\propto \pi(\vect{x} | \vect{\theta}) \pi(\vect{\theta}) \\
  &=  \frac{1}{\beta^5} \lambda_0^{y_0+1} \lambda_1^{y_1+1} \exp\left\{ -\lambda_0(t_1 - t_0) - \lambda_1(t_2 - t_1) - \frac{1 + \lambda_0 + \lambda_1}{\beta} \right\}
\end{align*}

Let $\theta^j$ denote the $j$-th element of the vector $\theta$, and let $\vect{\theta}^{-j} = \begin{pmatrix}\theta^0 & \dots & \theta^{j-1} & \theta^{j+1} & \dots & \theta^n\end{pmatrix}^T$  The full conditionals can then be written as

$$
  \pi(\theta^j | \vect{x}, \vect{\theta}^{-j}) = \frac{\pi(\vect{\theta} | \vect{x})}{\pi(\vect{\theta}^{-j} | \vect{x})} \propto  \pi(\vect{\theta} | \vect{x})
$$

Stated differently, the full conditionals can be directly derived from the posterior density $\pi(\vect{\theta} | \vect{x})$ by omitting all multiplicative factors which do not depend on $\theta^j$:

\begin{align*}
  \pi(t_1 | \vect{x}, \lambda_0, \lambda_1, \beta) &\propto \lambda_0^{y_0+1} \lambda_1^{y_1+1} \exp\left\{ t_1(\lambda_1 - \lambda_0) \right\} \quad &&\forall \quad t_1 \in [t_0, t_1] \\

  \pi(\lambda_0 | \vect{x}, t_1, \lambda_1, \beta) &\propto \lambda_0^{y_0+1} \exp\left\{-\lambda_0(t_1 - t_0 + \beta^{-1})\right\} \quad &&\forall \quad \lambda_0 \geq 0\\

  \pi(\lambda_1 | \vect{x}, t_1, \lambda_0, \beta) &\propto \lambda_1^{y_1 + 1} \exp\left\{-\lambda_1(t_2  - t_1 + \beta^{-1})\right\}, \quad &&\forall \quad \lambda_1 \geq 0 \\

  \pi(\beta | \vect{x}, t_1, \lambda_0, \lambda_1) &\propto \frac{1}{\beta^5} \exp\left\{ -\frac{1 + \lambda_0 + \lambda_1}{\beta} \right\} \quad &&\forall \quad \beta > 0.\\
\end{align*}

Observe that the full conditionals for $\lambda_0$ and $\lambda_1$ come from the Gamma distribution:

\begin{align*}
  [\lambda_0 | \vect{x}, t_1, \lambda_1, \beta] &\sim \text{Gamma}(y_0 + 2, 1/(t_1 - t_0 + \beta^{-1})) \\
  [\lambda_1 | \vect{x}, t_1, \lambda_0, \beta] &\sim \text{Gamma}(y_1 + 2, 1/(t_2 - t_1 + \beta^{-1}))
\end{align*}

Furthermore, the full conditional for $\beta$ comes from the Inverse Gamma distribution:

$$
  [\beta | \vect{x}, t_1, \lambda_0, \lambda_1] \sim \text{InvGamma}(4, 1/(1 + \lambda_0 + \lambda_1))
$$

We want to define and implement a single site MCMC algorithm for $\pi(\vect{\theta} | \vect{x})$. Since three of the full conditionals are standard, we will be using their distributions as proposals. This is known as Gibbs steps, and gives acceptance probabilities of 1. For the last full conditional, $[t_1 | \vect{x}, \lambda_0, \lambda_1, \beta]$, we will be sampling in Metropolis-Hastings (MH) steps. It can be shown that the MH algorithm converges to the target distribution regardless of the choice of proposal distribution. 

Define $\vect{\theta}_i$ to be the realization of our parameter vector $\vect{\theta}$ at the $i$-th MCMC step. Also, let $Q(\tilde{\theta}^j | \theta^j_{i-1}, \vect{\theta}^{-j}_{i-1})$ be the proposal distribution for the parameter $\theta^j$ at the $i$-th step. Then the acceptance probability $\alpha$ for the proposal $\tilde{\theta}^j$ at the $i$-th iteration can be found to be

$$
  \alpha(\tilde{\theta}^j | \vect{x}, \vect{\theta}_{i-1}) = \min\left( 1, \frac{\pi(\tilde{\theta}^j | \vect{x}, \vect{\theta}^{-j}_{i-1})}{\pi(\theta^j_{i-1} | \vect{x}, \vect{\theta}^{-j}_{i-1})} \times \frac{Q(\theta^j_{i-1} | \vect{x}, \tilde{\theta}^j, \vect{\theta}^{-j}_{i-1})}{Q(z^j | \vect{x},   \vect{\theta}_{i-1})} \right)
$$

From the expression above, one can confirm that the Gibbs steps give $\alpha = 1$, since the proposal is equal to the full conditional, and so all factors in the fractions cancel out. So a Gibbs step is just a special case of a Metropolis-Hastings step.

Since the full conditional for $t_1$ seems to resemble a distribution close to the exponential, it may be tempting to use an exponential independence proposal. However, recall that $y_0$ and $y_1$ are dependent on $t_1$, and so it is not close to an exponential at all. Therefore, $[t_1 | \vect{x}, \lambda_0, \lambda_1, \beta]$ should not be sampled by an exponential independence proposal.

Instead we consider the random walk proposal. Then $Q(\tilde{t_1} | \vect{x}, \vect{\theta}_{i-1}) = \theta^{t_1}_{i-1} + \pi(\epsilon)$, where $\epsilon$ comes from a 0-centered symmetrical distribution. In our case, we have chosen a Gaussian random walk, i.e.

$$
  [\tilde{t_1} | \vect{x}, \vect{\theta}_{i-1}] \sim \phi(\theta^{t_1}_{i-1}, \sigma^2_{t_1})
$$
where $\sigma^2_{t_1}$ is a tuning parameter. 

A weakness of the random walk proposal is that the Markov Chain may get stuck in a local maximum. There is no way of being certain that we are avoiding it, but by choosing $\alpha \in [0.2, 0.5]$ one at least reduces the risk for this to happen. This can be done by tuning $\sigma^2_{t_1}$. 

The proposal distribution for $t_1$ is symmetric around the current value, i.e. $Q(\tilde{\theta}^{t_1} | \vect{x}, \vect{\theta}_{i-1}) = Q(\theta^{t_1}_{i-1} | \vect{x}, \tilde{\theta}^{t_1}, \vect{\theta}^{-t_1}_{i-1})$. Hence,

\begin{align*}
  \alpha(\tilde{t_1} | \vect{x}, \vect{\theta}_{i-1}) &= \min\left(1, \frac{\pi(\tilde{t_1} | \vect{x}, \vect{\theta}^{-t_1}_{i-1})}{\pi(\theta^{t_1}_{i-1} | \vect{x}, \vect{\theta}^{-t_1}_{i-1})} \right) \\
  &= \lambda_0^{\tilde{y_0} - y_0} \lambda_1^{\tilde{y_1} - y_1} \exp\left\{(\tilde{t_1} - \theta^{t_1}_{i-1}) (\lambda_1 - \lambda_0)\right\}
\end{align*}


Here, $\tilde{y_0}$ and $\tilde{y_1}$ are the values for $y_0$ and $y_1$ with the proposed value $\tilde{t_1}$ respectively.

This single site Metropolis-within-Gibbs MCMC sampler has been implemented below:

```{R}
sample_lambda0_fc = function(t0, t1, beta, y0) {
  return(rgamma(1, shape=y0+2, scale=1/(t1 - t0 + 1/beta)))
}

sample_lambda1_fc = function(t1, t2, beta, y1) {
  return(rgamma(1, shape=y1+2, scale=1/(t2 - t1 + 1/beta)))
}

sample_beta_fc = function(lambda0, lambda1) {
  return(rinvgamma(1, shape=4, scale=1/(1 + lambda0 + lambda1)))
}

sample_t1_rw = function(t1_prev, lambda0, lambda1, t0, t2, x, sigma) {
  y0_prev = sum(x <= t1_prev)
  y1_prev = sum(x > t1_prev)
  
  t1_proposal = rnorm(1, t1_prev, sigma)
  if (t1_proposal <= t0 || t1_proposal >= t2) {
    # t1 is outside range, so reject it:
    return(list(
      t1=t1_prev,
      y0=y0_prev,
      y1=y1_prev,
      accepted=F
    ))
  }
  
  y0_proposal = sum(x <= t1_proposal)
  y1_proposal = sum(x > t1_proposal)
  
  alpha = min(1, lambda0^(y0_proposal - y0_prev) * lambda1^(y1_proposal - y1_prev) * exp((t1_proposal - t1_prev)*(lambda1 - lambda0)))
  
  u = runif(1)
  accept = u < alpha

  return(list(
    t1=ifelse(accept, t1_proposal, t1_prev),
    y0=ifelse(accept, y0_proposal, y0_prev),
    y1=ifelse(accept, y1_proposal, y1_prev),
    accepted=accept
  ))
}

run_mcmc_single = function(n_iterations, x, sigma, t1_start, lambda0_start, lambda1_start, beta_start) {
  t0 = x[1]
  t2 = tail(x, n=1)
  
  result = data.frame(
    t1=c(t1_start, rep(NA, n_iterations - 1)),
    lambda0=c(lambda0_start, rep(rep(NA, n_iterations-1))),
    lambda1=c(lambda1_start, rep(rep(NA, n_iterations-1))),
    beta=c(beta_start, rep(rep(NA, n_iterations-1))),
    t1_accepted=c(T, rep(NA, n_iterations-1))
  )
  
  # Storing the current parameters in a list is practical,
  # since we can change the sampling order without having to deal
  # with indexes
  curr_params = list(
    t1=t1_start,
    lambda0=lambda0_start,
    lambda1=lambda1_start,
    beta=beta_start
  )
  
  for(i in 1:n_iterations) {
    t1_result = sample_t1_rw(curr_params$t1, curr_params$lambda0, curr_params$lambda1, t0, t2, x, sigma)
    curr_params$t1 = t1_result$t1
    
    curr_params$lambda0 = sample_lambda0_fc(t0, curr_params$t1, curr_params$beta, t1_result$y0)
    curr_params$lambda1 = sample_lambda1_fc(curr_params$t1, t2, curr_params$beta, t1_result$y1)
    curr_params$beta = sample_beta_fc(curr_params$lambda0, curr_params$lambda1)
    
    result[i,] = c(curr_params, t1_accepted=t1_result$accepted)
  }
  
  result$iter = as.numeric(row.names(result))
  return(result)
}
```

Since we have a tuning parameter $\sigma^2_{t_1}$, it is interesting to evaluate the performance of the algorithm when varying it. 

```{R}
n_iterations = 50000
x = coal$date

# Tuning parameters
sigma_t1s = c(1, 5, 10, 20)

# Model parameters:
lambda0_start = 2
lambda1_start = 2
beta_start = sample_beta_fc(lambda0_start, lambda1_start)
t1_start = 1940

result.s_1 = run_mcmc_single(n_iterations, x, 1, t1_start, lambda0_start, lambda1_start, beta_start)
result.s_5 = run_mcmc_single(n_iterations, x, 5, t1_start, lambda0_start, lambda1_start, beta_start)
result.s_10 = run_mcmc_single(n_iterations, x, 10, t1_start, lambda0_start, lambda1_start, beta_start)
result.s_20 = run_mcmc_single(n_iterations, x, 20, t1_start, lambda0_start, lambda1_start, beta_start)
```
```{R}
plot.s_1 = ggplot(data=result.s_1, aes(x=iter, y=t1)) + geom_line()
plot.s_5 = ggplot(data=result.s_5, aes(x=iter, y=t1)) + geom_line()
plot.s_10 = ggplot(data=result.s_10, aes(x=iter, y=t1)) + geom_line()
plot.s_20 = ggplot(data=result.s_20, aes(x=iter, y=t1)) + geom_line()

ggarrange(ggarrange(plot.s_1, plot.s_5, ncol=2, labels=c("sigma=1", "sigma=5")),
          ggarrange(plot.s_10, plot.s_20, ncol=2, labels=c("sigma=10", "sigma=20")),
          nrow=2)

```

Above is the trace plots for the exploration of $t_1$ plotted for different values of $\sigma^2_{t_1}$. We have intentionally provided an initial value $\theta^{t_1}_0 = 1940$, since we know that this is a local maximum for $t_1$. Observe that with $\sigma_{t_1} = 1$, the algorithm gets stuck in this local maximum at $t_1 \approx 1940$ for almost over half of the time. This happens because the random walk process takes moves in quite small steps, and so the probability of taking long steps to escape the local maximum is really small. This means that the algorithm will have a really long burn-in for too low $\sigma_{t_1}$, and in the case of $\sigma_{t_1} = 1$, the burn-in was in this case around 35 000. We get an improvement with increasing $\sigma_{t_1}$ as expected, and for $\sigma_{t_1} \in \{5, 10\}$ we see that the Markov Chain explores the sample space pretty well. However, with even higher values such as $\sigma_{t_1} = 20$, we get new issues. Now the sampler is proposing samples way outsides our modes, and so we get a lot of rejections. This means that it will stay at the same value for a long time before moving. It may be a bit difficult to observe in the plot above, so we have zoomed in on the traceplot for $\sigma_{t_1} \in \{5, 20\}$ in the figure below:

```{R}
plot.s_5_zoomed = ggplot(data=result.s_5[10000:10500,], aes(x=iter, y=t1)) + geom_line()
plot.s_20_zoomed = ggplot(data=result.s_20[10000:10500,], aes(x=iter, y=t1)) + geom_line()
ggarrange(plot.s_5_zoomed, plot.s_20_zoomed, ncol=2, labels=c("sigma=5", "sigma=20"))
```

Here it is quite prominent that we have to reject up to 50 samples before we get a new accepted proposal. This makes the algorithm really inefficient, and so we know from this that $\sigma_{t_1} < 20$ in order for the algorithm to have an acceptable acceptance rate.. 


Below we have calculated the acceptance rates for each value of $\sigma_{t_1}$. We see that $\alpha(\tilde{t_1} | \vect{x}, \vect{\theta}_{i-1}) \notin [0.2, 0.5] \quad \forall \quad \sigma_{t_1} \in \{1, 10, 20\}$. Combined with the traceplots above, we conclude with that $\sigma_{t_1}$ gives best mixing properties for $t_1$, and is a good value to use. 
```{R}
acceptances = data.frame("sigma", "acceptance")
acceptances[nrow(acceptances)+1,] = c(1, length(result.s_1$t1_accepted[result.s_1$t1_accepted == T]) / nrow(result.s_1))
acceptances[nrow(acceptances)+1,] = c(5, length(result.s_5$t1_accepted[result.s_5$t1_accepted == T]) / nrow(result.s_5))
acceptances[nrow(acceptances)+1,] = c(10, length(result.s_10$t1_accepted[result.s_10$t1_accepted == T]) / nrow(result.s_10))
acceptances[nrow(acceptances)+1,] = c(20, length(result.s_20$t1_accepted[result.s_20$t1_accepted == T]) / nrow(result.s_20))
print(acceptances)
```
This can also be seen in the traceplots below. Here we see that the algorithm explores the values for $\vect{\theta}$ quite well - it does not get stuck at some maxima, and it jumps quickly between its extreme values:

```{R}
show_traceplots = function(results, text)  {
  annotate_figure(
  ggarrange(
    ggarrange(
      ggplot(data=results, aes(x=iter, y=t1)) + geom_line(),
      ggplot(data=results, aes(x=iter, y=beta)) + geom_line(),
    ncol=2),
    ggarrange(
      ggplot(data=results, aes(x=iter, y=lambda0)) + geom_line(),
      ggplot(data=results, aes(x=iter, y=lambda1)) + geom_line(),
    ncol=2), nrow=2),
  top = text_grob(text))
}
show_traceplots(result.s_5, "Traceplots for sigma_t1 = 5")
```

It should however be noted that the choice of $\sigma_{t_1}$ should \textit{not} affect the limiting distribution, and we want to prove this. Below we have plotted the limiting distribution for the full conditional of $t_1$ with $\sigma_{t_1} \in \{5, 10\}$. We used these values because they give a high enough acceptance rate so that we do not need to run the algorithms for several minutes in order for the distributions to converge. By limiting distribution we mean the distribution we get from the samples when we neglect the samples from the burn-in period. Let the burn-in period be 3000 samples for both of them. Then the limiting distributions will be as follows:

```{R}
burnin = 3000
ggplot() + 
  geom_density(data=result.s_5[burnin:nrow(result.s_5),], aes(x=t1), colour="red") +
  geom_density(data=result.s_10[burnin:nrow(result.s_10),], aes(x=t1), colour="blue")
```

We do see some differences between these distributions, e.g. in the second-highest mode and the leftmost mode. But recall that MCMC only guarantees _eventual_ convergence, and therefore there will also be some small differences between the limiting distributions for different values of tuning parameters, random seeds etc. The plots above do however prove the fact that they converge to the same distributions, as they are quite similar. 

Now that we have evaluated burn-in, mixing properties and the fact that the choice of $\sigma_{t_1}$ does not affect the limiting distribution, we have actually left off maybe the most important question; how does the estimated model parameters match the data that we provided? In order to find an estimate of $\E[\vect{\theta} | \vect{x}]$ we would ideally have used a Maximum Likelihood estimator on the data. But since this project is all about MCMC, we have simplified the problem a bit and will assume that the ML estimate can be approximated to the sample mean for each of the model parameters. Below we have calculated these estimated and superimposed the expectation lines which ensembles the estimates on top of the cumulative data. We have used the samples obtained from using $\sigma_{t_1} = 10$.

```{R}
burnin = 3000
t0 = x[1]
t1 = mean(result.s_5[burnin:nrow(result.s_5),]$t1)
t2 = tail(x, n=1)
lambda0 = mean(result.s_5[burnin:nrow(result.s_5),]$lambda0)
lambda1 = mean(result.s_5[burnin:nrow(result.s_5),]$lambda1)

y0 = 0
y1 = lambda0 * (t1 - t0)
y2 = y1 + lambda1 * (t2 - t1)

ggplot() +
  geom_point(data=coal_by_year, aes(x=year, y=cum_n, group=1, colour="data")) +
  geom_segment(aes(x=t0, y=y0, xend=t1, yend=y1, colour="estimate")) +
  geom_segment(aes(x=t1, y=y1, xend=t2, yend=y2, colour="estimate")) +
  ggtitle("Cumulative number of accidents in coal-mines vs year")
```

From the above plot, we see that the estimates matches the data pretty well, and so we conclude that our single site Metropolis-within-Gibbs MCMC sampler works as intended.

We now want to implement MCMC using the following block proposals, where we will be alternating between the two proposals for each iteration:

* $(t_1, \lambda_0, \lambda_1)$ keeping $\beta$ unchanged
* $(\beta, \lambda_0, \lambda_1)$ keeping $t_1$ unchanged
  
For the first block, we start by sampling our proposal $\tilde{t_1}$ using the Gaussian random walk as done in our single-site MCMC algorithm. Then $\tilde{\lambda_0}$ and $\tilde{\lambda_1}$ can be sampled from their joint conditional conditioned on the proposed $\tilde{t_1}$, $\pi(\tilde{\lambda_0}, \tilde{\lambda_1} | \vect{x}, \tilde{t_1}, \beta)$. Recognize that the non-normalized joint conditional can be found by omitting all multiplicative factors from $\pi(\tilde{t_1}, \tilde{\lambda_0}, \tilde{\lambda_1}, \beta | \vect{x})$ that do not depend on $\lambda_0$ and $\lambda_1$. Hence,

\begin{align*}
  \pi(\lambda_0, \lambda_1 | \vect{x}, \tilde{t_1}, \beta) &\propto \lambda_0^{\tilde{y_0} + 1} \lambda_1^{\tilde{y_1} + 1} \exp\left\{-\lambda_0(t_1 - t_0) -\lambda_1(t_2 - \tilde{t_1}\right\} \\
  &\propto \pi(\lambda_0 | \vect{x}, \tilde{t_1}, \lambda_1, \beta) \times \pi(\lambda_1 | \vect{x}, \tilde{t_1}, \lambda_0, \beta) \\
  &\Rightarrow [\lambda_0 | \vect{x}, \tilde{t_1}, \lambda_1, \beta] \indep [\lambda_1 | \vect{x}, \tilde{t_1}, \lambda_0, \beta]
\end{align*}

We can sample the joint conditional by independently sampling $\tilde{\lambda_0}$ and $\tilde{\lambda_1}$ from their full conditionals using the proposed $\tilde{t_1}$, as they both are standard distributions. The acceptance probability is expressed as:

$$
  \alpha(\tilde{t_1}, \tilde{\lambda_0}, \tilde{\lambda_1} | \vect{x}, \vect{\theta}_{i-1}) = \min \left( 1, \frac{\pi(\tilde{t_1}, \tilde{\lambda_0}, \tilde{\lambda_1} | \vect{x}, \beta)}{\pi(t_1, \lambda_0, \lambda_1 | \vect{x}, \beta)} \times \frac{Q(t_1, \lambda_0, \lambda_1 | \vect{x}, \tilde{t_1}, \tilde{\lambda_0}, \tilde{\lambda_1}, \beta)}{Q(\tilde{t_1}, \tilde{\lambda_0}, \tilde{\lambda_1} | \vect{x}, t_1, \lambda_0, \lambda_1)}\right)
$$

Since $\tilde{t_1} \indep [\tilde{\lambda_0}, \tilde{\lambda_1}]$, it follows that

\begin{align*}
  Q(\tilde{t_1}, \tilde{\lambda_0}, \tilde{\lambda_1} | \vect{x}, t_1, \lambda_0, \lambda_1) &= Q(\tilde{\lambda_0}, \tilde{\lambda_1} | \vect{x}, \tilde{t_1}, \beta) \times Q(\tilde{t_1} | \vect{x}, t_1, \beta) \\
  &= \pi(\tilde{\lambda_0} | \vect{x}, \tilde{t_1}, \beta) \times \pi(\tilde{\lambda_1} | \vect{x}, \tilde{t_1}, \beta) \times \phi(t_1, \sigma^2_{t_1})
\end{align*}
$$
  = \frac{\tilde{\lambda_0}^{\tilde{y_0} + 1}(\tilde{t_1} - t_0 + \beta^{-1})^{\tilde{y_0} + 2} e^{-\tilde{\lambda_0}(\tilde{t_1} - t_0 + \beta^{-1})}}{\Gamma(\tilde{y_0} + 2)} \times \frac{\tilde{\lambda_1}^{\tilde{y_1} + 1}(t_2 - \tilde{t_1} + \beta^{-1})^{\tilde{y_1} + 2} e^{-\tilde{\lambda_1}(t_2 - \tilde{t_1} + \beta^{-1})}}{\Gamma(\tilde{y_1} + 2)} \times \phi(t_1, \sigma^2_{t_1})
$$

The posterior density $\pi(t_1, \lambda_0, \lambda_1 | \vect{x}, \beta)$ up to a proportionality constant is found by marginalizing $\pi(\vect{\theta} | \vect{x})$, i.e.:

$$
  \pi(t_1, \lambda_0, \lambda_1 | \vect{x}, \beta) \propto \lambda_0^{y_0 + 1} \lambda_1^{y_1 + 1} \exp\left\{-\lambda_0(t_1 - t_0) - \lambda_1(t_2 - t_1) - \frac{1 + \lambda_0 + \lambda_1}{\beta}\right\}
$$

Notice that $Q(t_1, \lambda_0, \lambda_1 | \vect{x}, \tilde{t_1}, \tilde{\lambda_0}, \tilde{\lambda_1}, \beta) = Q(\tilde{t_1}, \tilde{\lambda_0}, \tilde{\lambda_1} | \vect{x}, t_1, \lambda_0, \lambda_1)$ and $\pi(\tilde{t_1}, \tilde{\lambda_0}, \tilde{\lambda_1} | \vect{x}, \beta) = \pi(t_1, \lambda_0, \lambda_1 | \vect{x}, \beta)$, and so they will be on the same form as the above expressions. Inserting these into $\alpha(\tilde{t_1}, \tilde{\lambda_0}, \tilde{\lambda_1} | \vect{x}, \vect{\theta}_{i-1})$, we get

$$
  \alpha(\tilde{t_1}, \tilde{\lambda_0}, \tilde{\lambda_1} | \vect{x}, \vect{\theta}_{i-1}) = \min \left( 1, \frac{(t_1 - t_0 + \beta^{-1})^{y_0 + 2}(t_2 - t_1 + \beta^{-1})^{y_1 + 2}}{(\tilde{t_1} - t_0 + \beta^{-1})^{\tilde{y_0} + 2}(t_2 - \tilde{t_1} + \beta^{-1})^{\tilde{y_1} + 2}} \times \frac{\Gamma(\tilde{y_0} + 2) \Gamma(\tilde{y_1} + 2)}{\Gamma(y_0 + 2)\Gamma(y_1 + 2)} \right)
$$
Let's now check if this equation can be used in a numerical setting. From our single-site MCMC, we estimated $\hat{t_1} = 1890$, $\hat{\beta} = 3$, with $t_0 = 1850$. Then we get $\hat{y_0} = 130$, and so $(\hat{t_1} - \hat{t_0} + \hat{\beta}^{-1})^{\hat{y_0} + 2} \approx 2.964277\text{E+}211$. This will lead to numerical overflow, and so it should be calculated in $\log$-scale instead.

In the second block, we start by sampling $[\tilde{\beta} | \vect{x}, \vect{\theta}^{-\beta}_{i-1}] \sim \phi(\beta, \sigma^2_\beta)$. Then we use this proposal $\tilde{\beta}$ to sample $[\tilde{\lambda_0}, \tilde{\lambda_1} | \vect{x}, t_1, \tilde{\beta}] \sim \pi(\lambda_0, \lambda_1 | \vect{x}, t_1, \tilde{\beta})$. Notice that  $\pi(\lambda_0, \lambda_1 | \vect{x}, \tilde{t_1}, \beta) =  \pi(\lambda_0, \lambda_1 | \vect{x}, t_1, \tilde{\beta})$, so we already have an expression for sampling $\tilde{\lambda_0}$ and $\tilde{\lambda_1}$. For the acceptance probability, we have that

$$
  \alpha(\tilde{\lambda_0}, \tilde{\lambda_1}, \tilde{\beta} | \vect{x}, \vect{\theta}_{i-1}) = \min \left( 1, \frac{\pi(\tilde{\lambda_0}, \tilde{\lambda_1}, \tilde{\beta} | \vect{x}, t_1)}{\pi(\lambda_0, \lambda_1, \beta | \vect{x}, t_1)} \times \frac{Q(\lambda_0, \lambda_1, \beta | \vect{x}, t_1, \tilde{\lambda_0}, \tilde{\lambda_1}, \tilde{\beta})}{Q(\tilde{\lambda_0}, \tilde{\lambda_1}, \tilde{\beta} | \vect{x}, t_1, \lambda_0, \lambda_1, \beta)}\right)
$$

Similarly to what was the case for the first block,

\begin{align*}
  Q(\tilde{\lambda_0}, \tilde{\lambda_1}, \tilde{\beta} | \vect{x}, t_1, \lambda_0, \lambda_1, \beta) &= Q(\tilde{\lambda_0}, \tilde{\lambda_1} | \vect{x}, t_1, \tilde{\beta}) \times \phi(\beta, \sigma^2_\beta) \\
  &= \pi(\tilde{\lambda_0} | \vect{x}, t_1, \tilde{\beta}) \times \pi(\tilde{\lambda_0} | \vect{x}, t_1, \tilde{\beta}) \times \phi(\beta, \sigma^2_\beta)
\end{align*}

It then follows that

$$
  Q(\tilde{\lambda_0}, \tilde{\lambda_1} | \vect{x}, t_1, \tilde{\beta}) = \frac{\tilde{\lambda_0}^{y_0 + 1} (t_1 - t_0 + \tilde{\beta}^{-1})^{y_0 + 2} e^{-\tilde{\lambda_0}(t_1 - t_0 + \tilde{\beta}^{-1})}}{\Gamma(y_0 + 2)} \times \frac{\tilde{\lambda_1}^{y_1 + 1} (t_2 - t_1 + \tilde{\beta}^{-1})^{y_1 + 2} e^{-\tilde{\lambda_1}(t_2 - t_1 + \tilde{\beta}^{-1})}}{\Gamma(y_1 + 2)} 
$$

By marginalizing $\pi(\vect{\theta} | \vect{x})$, we get
$$
  \pi(\lambda_0, \lambda_1, \beta | \vect{x}, t_1) \propto \frac{1}{\beta^5} \lambda_0^{y_0 + 1} \lambda_1^{y_1 + 1} \exp\left\{-\lambda_0\left(t_1 - t_0 + \beta^{-1}\right)-\lambda_1\left(t_2 - t_1 + \beta^{-1}\right)\right\}
$$

The same can be done for $\pi(\tilde{\lambda_0}, \tilde{\lambda_1}, \tilde{\beta} | \vect{x}, t_1)$ and $Q(\lambda_0, \lambda_1, \beta | \vect{x}, t_1, \tilde{\lambda_0}, \tilde{\lambda_1}, \tilde{\beta})$. Inserting these expressions into the equation for the acceptance probability, we end up with

$$
  \alpha(\tilde{\lambda_0}, \tilde{\lambda_1}, \tilde{\beta} | \vect{x}, \vect{\theta}_{i-1}) = \min \left( 1, \frac{\beta^5}{\tilde{\beta}^5} \cdot \frac{(t_1 - t_0 + \beta^{-1})^{y_0 + 2}(t_2 - t_1 + \beta^{-1})^{y_1 + 2}}{(t_1 - t_0 + \tilde{\beta}^{-1})^{y_0 + 2}(t_2 - t_1 + \tilde{\beta}^{-1})^{y_1 + 2}}\right)
$$
Also for this $\alpha$ we have factors which would lead to numerical overflow, so we calculate this acceptance probability in $\log$-scale.

Notice that we by using these block proposals are proposing samples for $\lambda_0$ and $\lambda_1$ for each iteration, whereas $t_1$ and $\beta$ are only proposed samples every other iteration. This is in theory not an issue, as MCMC still guarantees eventual convergence. But we do have to make sure that they both are converging by tuning $\sigma^2_{t_1}$ and $\sigma^2_\beta$ such that  $\left\{\alpha(\tilde{\lambda_0}, \tilde{\lambda_1}, \tilde{\beta} | \vect{x}, \vect{\theta}_{i-1}), \alpha(\tilde{\lambda_0}, \tilde{\lambda_1}, \tilde{\beta} | \vect{x}, \vect{\theta}_{i-1})\right\} \in [0.2, 0.5]$ as previously mentioned.

An implementation of the proposed block Metropolis-Hastings algorithm has been provided in the listing below.

```{R}
sample_first_block = function(curr_params, x, t0, t2, sigma_t1) {
  t1_proposal = rnorm(1, curr_params$t1, sigma_t1)
  # Should we reject here instead?
  if(t1_proposal > t2 || t1_proposal < t0) {
    t1_proposal = curr_params$t1
  }
  
  y0_prev = sum(x <= curr_params$t1)
  y1_prev = sum(x > curr_params$t1)
  
  y0_proposal = sum(x <= t1_proposal)
  y1_proposal = sum(x > t1_proposal)
  
  lambda0_proposal = sample_lambda0_fc(t0, t1_proposal, curr_params$beta, y0_proposal)
  lambda1_proposal = sample_lambda1_fc(t1_proposal, t2, curr_params$beta, y1_proposal)
  
  alpha_first_part_log = 
      (y0_prev + 2)*log(curr_params$t1 - t0 + 1/curr_params$beta) +
      (y1_prev + 2)*log(t2 - curr_params$t1 + 1/curr_params$beta) -
      (y0_proposal + 2)*log(t1_proposal - t0 + 1/curr_params$beta) -
      (y1_proposal + 2)*log(t2 - t1_proposal + 1/curr_params$beta)
  
  alpha_second_part_log = lgamma(y0_proposal + 2) + lgamma(y1_proposal + 2) - lgamma(y0_prev + 2)  - lgamma(y1_prev + 2)
  
  alpha = min(1, exp(alpha_first_part_log + alpha_second_part_log))
  u = runif(1)
  
  if(u < alpha) {
    # Accept
    curr_params$t1 = t1_proposal
    curr_params$lambda0 = lambda0_proposal
    curr_params$lambda1 = lambda1_proposal
    curr_params$accepted = T
    return(curr_params)
  } else {
    # Reject
    curr_params$accepted = F
    return(curr_params)
  }
}

sample_second_block = function(curr_params, x, t0, t2, sigma_beta) {
  beta_proposal = rnorm(1, curr_params$beta, sigma_beta)
  # Should we reject here instead?
  if(beta_proposal <= 0) {
    beta_proposal = curr_params$beta
  }
  
  y0_prev = sum(x <= curr_params$t1)
  y1_prev = sum(x > curr_params$t1)
  
  lambda0_proposal = sample_lambda0_fc(t0, curr_params$t1, beta_proposal, y0_prev)
  lambda1_proposal = sample_lambda1_fc(curr_params$t1, t2, beta_proposal, y1_prev)
  
  alpha_log =
      5*(log(curr_params$beta) - log(beta_proposal)) +
      (y0_prev + 2)*(log(curr_params$t1 - t0 + 1/curr_params$beta) -
        log(curr_params$t1 - t0 + 1/beta_proposal)) +
      (y1_prev + 2)*(log(t2 - curr_params$t1 + 1/curr_params$beta) -
        log(t2 - curr_params$t1 + 1/beta_proposal))
  
  alpha = min(1, exp(alpha_log))
  u = runif(1)
  
  if(u < alpha) {
    # Accept
    curr_params$beta = beta_proposal
    curr_params$lambda0 = lambda0_proposal
    curr_params$lambda1 = lambda1_proposal
    curr_params$accepted = T
    return(curr_params)
  } else {
    # Reject
    curr_params$accepted=F
    return(curr_params)
  }
}

run_mcmc_block = function(n_iterations, x, sigma_t1, sigma_beta, t1_start, lambda0_start, lambda1_start, beta_start) {
  t0 = x[1]
  t2 = tail(x, n=1)
  
  result = data.frame(
    t1=c(t1_start, rep(NA, n_iterations - 1)),
    lambda0=c(lambda0_start, rep(rep(NA, n_iterations-1))),
    lambda1=c(lambda1_start, rep(rep(NA, n_iterations-1))),
    beta=c(beta_start, rep(rep(NA, n_iterations-1))),
    accepted=c(T, rep(NA, n_iterations-1))
  )
  
  # Storing the current parameters in a list is practical,
  # since we can change the sampling order without having to deal
  # with indexes
  curr_params = list(
    t1=t1_start,
    lambda0=lambda0_start,
    lambda1=lambda1_start,
    beta=beta_start
  )
  
  for(i in 1:n_iterations) {
    if(i %% 2 == 1) {
      curr_params = sample_first_block(curr_params, x, t0, t2, sigma_t1)
    } else {
      curr_params = sample_second_block(curr_params, x, t0, t2, sigma_beta)
    }
    result[i,] = curr_params
  }
  
  result$iter = as.numeric(row.names(result))
  return(result)
}
```

We now want to see how the algorithm performs when varying the tuning parameters $\sigma_\beta$. For simplicity, we have left $\sigma_{t_1}$ at $\sigma_{t_1} = 5$. Since we now have two random walk proposals, both $\beta$ and $\t_1$ have been plotted in trace-plots below:  

```{R}
n_iterations = 50000
x = coal$date

# Tuning parameters
sigma_t1 = 5

# Model parameters:
lambda0_start = 5
lambda1_start = 5
beta_start = sample_beta_fc(lambda0_start, lambda1_start)
t1_start = 1940

# Calculate
result.sb_01 = run_mcmc_block(n_iterations, x, sigma_t1, 0.1, t1_start, lambda0_start, lambda1_start, beta_start)
result.sb_1 = run_mcmc_block(n_iterations, x, sigma_t1, 1, t1_start, lambda0_start, lambda1_start, beta_start)
result.sb_10 = run_mcmc_block(n_iterations, x, sigma_t1, 10, t1_start, lambda0_start, lambda1_start, beta_start)
```

```{R}
annotate_figure(ggarrange(
  ggplot(data=result.sb_01, aes(x=iter, y=t1)) + geom_line(),
  ggplot(data=result.sb_01, aes(x=iter, y=beta)) + geom_line(),
  ncol=2, nrow=1), 
  top = text_grob("Traceplots with sigma_t1 = 5, sigma_beta=0.1"))
annotate_figure(ggarrange(
  ggplot(data=result.sb_1, aes(x=iter, y=t1)) + geom_line(),
  ggplot(data=result.sb_1, aes(x=iter, y=beta)) + geom_line(),
  ncol=2, nrow=1), 
  top = text_grob("Traceplots with sigma_t1 = 5, sigma_beta=5"))
annotate_figure(ggarrange(
  ggplot(data=result.sb_10, aes(x=iter, y=t1)) + geom_line(),
  ggplot(data=result.sb_10, aes(x=iter, y=beta)) + geom_line(),
  ncol=2, nrow=1), 
  top = text_grob("Traceplots with sigma_t1 = 5, sigma_beta=10"))
```

From the above traceplots, we see that $t_1$ is explored in a similar fashion for all choices of $\sigma_\beta$. This is to be expected, since they all use the same $\sigma_{t_1} = 5$. However, we do see that $\beta$ is explored quite slowly with $\sigma_\beta = 0.1$. In order to get an expression on why this happens, we can evaluate the acceptance rates as listed below. We see that for $\sigma_\beta = 0.1$, we get an acceptance rate above the recommended $\alpha \in [0.2, 0.5]$. In fact, the block proposal for $(\beta, \lambda_0, \lambda_0)$ probably has an acceptance rate much higher than this, since we know that the random walk proposal from the single-site MCMC had an acceptance rate around 0.3 with the same tuning parameter $\sigma_{t_1}$. In practice, it means that this second block proposal accepts the majority of the proposal samples, indicating that it takes long time for it to explore the whole sample space. Therefore, $\sigma_\beta = 0.1$ is a too low value. $\sigma_\beta = 10$ seems to be much better with acceptance rate of $0.42$, and this is the value we will use from now on.
```{R}
acceptances_b = data.frame("sigma_beta", "acceptance")
acceptances_b[nrow(acceptances_b)+1,] = c(0.1, length(result.sb_01$accepted[result.sb_01$accepted == T]) / nrow(result.sb_01))
acceptances_b[nrow(acceptances_b)+1,] = c(1, length(result.sb_1$accepted[result.sb_1$accepted == T]) / nrow(result.sb_1))
acceptances_b[nrow(acceptances_b)+1,] = c(10, length(result.sb_10$accepted[result.sb_10$accepted == T]) / nrow(result.sb_10))

print(acceptances_b)
```

We continue our evaluation by looking at all the traceplots for the parameters. These plots have been provided in the figure below. Compared to the single-site MCMC, we get pretty similar results. The main difference is that $\beta$ has more extreme samples than what we got in single-site. But in total, the mixing properties are good for them both.

```{R}
show_traceplots(result.sb_10, "Traceplots with sigma_t1 = 5, sigma_beta=10")
```

They do however differ in terms of their burn-in period. Looking at the plot below, the block MH algorithm approaches the correct distribution for $t_1$ after only a couple of samples, whereas the single-site MH algorithm uses more than 500 for its burn-in period. In our case this is not a severe difference, but in other cases we might get huge differences here which affects the efficiency a lot. 

```{R}
ggarrange(
  ggplot(data=result.s_5[1:2000,], aes(x=iter, y=t1)) + geom_line() + ggtitle("Single-site"),
  ggplot(data=result.sb_10[1:2000,], aes(x=iter, y=t1)) + geom_line() + ggtitle("Block"),
  ncol=2, nrow=1)
```

Up until now, the only marginal posterior we have been plotting is that of $t_1$. Below, all marginal posteriors have been plotted by using the samples obtained from the block MH algorithm with $\sigma_{t_1} = 5$ and $\sigma_\beta = 10$. Notice that for $\lambda_0$, $\lambda_1$ and $\beta$, we can find the estimated marginal posteriors by using the means of the samples for $\vect{\theta}$ in order to obtain the corresponding theoretical distributions. Below we have plotted histograms of all the samples for each of the parameters, and then superimposed their posterior marginals obtained from standard distributions wherever applicable. Observe that the histograms matches the theoretical distributions very well. We do however have a slightly biased estimate for $[\beta | \vect{x}] $. Whether this is caused by numerical issues or if it happens because the chain has not run for a long enough time has not been investigated. 

```{R}
burnin = 1000
results = result.sb_10[burnin:nrow(result.sb_10),]

t0 = x[1]
t1 = mean(results[burnin:nrow(results),]$t1)
t2 = tail(x, n=1)
lambda0 = mean(results[burnin:nrow(results),]$lambda0)
lambda1 = mean(results[burnin:nrow(results),]$lambda1)
beta = mean(results[burnin:nrow(results),]$beta)

y0 = sum(x <= t1)
y1 = sum(x > t1)
annotate_figure(
  ggarrange(
    ggarrange(
      ggplot(data=results, aes(x=t1)) + geom_density(),
      ggplot(data=results, aes(x=beta)) + geom_density() + xlim(0, 7) +
        stat_function(fun=dinvgamma, args=list(shape=4, scale=1/(lambda0 - lambda1 + 1))),
    ncol=2),
    ggarrange(
      ggplot(data=results, aes(x=lambda0)) + geom_density() +
        stat_function(fun=dgamma, args=list(shape=y0 + 2, scale=1/(t1 - t0 + 1/beta))),
      ggplot(data=results, aes(x=lambda1)) + geom_density() +
        stat_function(fun=dgamma, args=list(shape=y1 + 2, scale=1/(t2 - t1 + 1/beta))),
  ncol=2), nrow=2),
top = text_grob("Marginal posteriors using sigma_t1=5, sigma_beta=10"))
```

Lastly, let's look at the covariance between $\lambda_0$ and $\lambda_1$. This has been estimated below, and we see that it is approximately zero as expected. Furthermore, the estimated expected values for the marginal posteriors of the model parameters have been provided below:

```{R}
sprintf("Estimate of E[t1 | x]: %f", t1)
sprintf("Estimate of E[lambda0 | x]: %f", lambda0)
sprintf("Estimate of E[lambda1 | x]: %f", lambda1)
sprintf("Estimate of E[beta | x]: %f", beta)
```

```{R}
covariance = cov(results$lambda0, results$lambda1)
sprintf("Covariance of lambda0 and lambda1: %f", covariance)
```

# Problem B: INLA for Gaussian Data
The plot below shows Gaussiandata.txt. The goal of this task is to implement INLA for a model with gaussian likelihood where computations can be done exactly.
```{R}
# Plot gaussian data

gaussian_data = read.delim("Gaussiandata.txt")
y = gaussian_data[,1]
t = seq(from=1, to=length(y), by=1)

plot(t, y)


```

## 1

Given the vector of linear predictors $\boldsymbol\eta=(\eta_1,\ldots,\eta_T)$, the observations $y_t$ are independent and Gaussian distributed with mean $\eta_t$ and known unit variance:

$$
  y_t \mid \eta_t \sim \mathcal{N}(\eta_t,1)\quad, for \space t = 1, \space ..., \space T
$$

The linear predictor $\eta_t$ for time $t$ is $\eta_t = f_t$

A second order random walk model as prior distribution for the vector $\mathbf{f}=(f_1,\ldots,f_T)$ is

$$
  \pi(\mathbf{f}\mid\theta) \propto \theta^{(T-2)/2} \text{exp}\Big\{-\frac{\theta}{2} \sum_{t=3}^T (f_t-2f_{t-1}+f_{t-2}^2)^2 \Big\}=\mathcal{N}(\mathbf{0},\mathbf{Q}(\theta)^{-1}).
$$

$\mathbf{Q}$ is the precision matrix and the precision parameter $\phi$ controls the smoothness of the vector f. Choosing a prior distribution for $\phi$ as the Gamma distribution with parameters 1 and 1.

This model could also be written as a hierarchial model:
$$
  \begin{aligned}
    \mathbf{y}\mid\mathbf{f} &\sim \prod_{t=1}^T P(y_t\mid \eta_t) \\
    \mathbf{f}\mid\theta &\sim \pi(\mathbf{f}\mid\theta) = \mathcal{N}(\mathbf{0},\mathbf{Q}(\theta)^{-1}) \\
    \theta &\sim \textrm{Gamma}(1,1)
  \end{aligned}
$$
The first line is the probability of $\mathbf{y}=(y_1,\ldots,y_T)$, the second line is the prior of the latent field, while the third line is the prior of the hyperparameter $\eta_t$.

This model is a latent Gaussian model, because the latent field is Gaussian distributed.

INLA could be used to estimate the parameters because the model is latent gausssian, where each data point $y_t$ only depends on one element of $f_t$ in the latent field, and because the precision matrix of the GMRF is sparse.



## 2

Implementing a Gibbs sampling algorithm for $f(\boldsymbol\eta,\theta\mid \mathbf{y})$ with a posterior as:


$$
\begin{align}
  \pi(\boldsymbol\eta,\theta\mid\mathbf{y}) & \propto \pi(\theta) \pi(\boldsymbol\eta\mid\theta) \prod_{t=1}^T\pi(y_t\mid\eta_t,\theta) \\ 
  & \propto \frac{\theta^{(T-2)/2}}{(2\pi)^{T/2}} \exp\bigg\{-\theta -\frac{\theta}{2}\sum_{t=3}^T(\eta_t-2\eta_{t-1}+\eta_{t-2})^2 -\frac{1}{2}\sum_{t=1}^T(y_t-\eta_t)^2 \bigg\}.
  \end{align}
$$

The full conditional for $\theta$ is then:
$$
  \begin{aligned}
    \pi(\theta\mid\mathbf{y},\boldsymbol\eta) &\propto \theta^{T/2-1} \exp\bigg\{-\theta\bigg(1+\frac{1}{2} \sum_{t=3}^T(\eta_t-2\eta_{t-1}+\eta_{t-2})^2\bigg) \bigg\} \\
    &\propto \textrm{Gamma}\bigg(\frac{T}{2}, 1+\frac{1}{2} \sum_{t=3}^T(\eta_t-2\eta_{t-1}+\eta_{t-2})^2 \bigg)
  \end{aligned}.
$$


The full conditional for $\eta$ is then:

$$
  \begin{aligned}
    \pi(\boldsymbol\eta\mid\theta,\mathbf{y}) &\propto \exp\bigg\{-\frac{\theta}{2} \sum_{t=3}^T(\eta_t-2\eta_{t-1}+\eta_{t-2})^2 -\frac{1}{2}\sum_{t=1}^T(y_t-\eta_t)^2 \bigg\} \\
    &= \exp\bigg\{-\frac{1}{2}\bigg(\boldsymbol\eta^T\mathbf{Q}\boldsymbol\eta+ (\mathbf{y}-\boldsymbol\eta)^T(\mathbf{y}-\boldsymbol\eta)  \bigg)\bigg\}\\
    &= \exp\bigg\{-\frac{1}{2}\boldsymbol\eta^T(\mathbf{Q}+\mathbf{I})\boldsymbol\eta+\mathbf{y}^T\boldsymbol\eta \bigg\}
  \end{aligned}.
$$

TODO: Precision matrix


```{R}
library(Matrix)

make.Q = function(T) {
  # Function to construct precision matrix
  diagonals = list(rep(6, T), rep(-4, T-1), rep(1, T-2))
  Q = as.matrix(bandSparse(T, T, k=c(0:2), diag=diagonals, symm=T))
  Q[1, 1] = Q[T, T] = 1
  Q[1, 2] = Q[2, 1] = Q[T, T-1] = Q[T-1, T] = -2
  Q[2, 2] = Q[T-1, T-1] = 5
  return (Q)
}

Q = make.Q(20)

```
The following function is implemented as a Gibbs sampler for $\pi(x,\theta|y)$. A burnin of 100 is chosen as this gives satisfying results.

```{R}
library(mvtnorm)
library(MASS)

set.seed(0)

sample.gibbs = function(n, theta.init, f.init, y) {
  #Create n-samples with gibbs-sampling

  T = length(y)
  x = rep(0, T) #initial x
  
  # Create empty vectors
  theta.vec = rep(0, n)
  f.matrix = matrix(1:T*n, nrow = T, ncol = n)

  theta.vec[1] = theta.init #Initialize vector by changing first element
  f.matrix[, 1] = f.init
  
  #Iterate MCMC loop
  for (i in 2:n) {
    # Update theta and sample
    sum = 0
    for(t in 3:T) {
      sum = sum + (f.matrix[t, i-1] - 2*f.matrix[t-1, i-1] + f.matrix[t-2, i-1])^2
    }
    theta.vec[i] = rgamma(1, shape = T/2, rate = 1 + 0.5*sum)

    # Update x
    Sigma = solve(theta.vec[i]*Q+diag(T))
    mu = Sigma%*%y
    f.matrix[, i] = c(rmvnorm(1,mean=mu, sigma=Sigma))
  }

  # Return matrix with f and theta
  return(rbind(f.matrix, theta.vec)) 

}

n = 10100 #Steps in MC
T = length(y)
theta.init = 0 #initial theta
f.init = rep(2, T)
Q = make.Q(T)

# Sample gibbs
samples = sample.gibbs(n, theta.init, f.init, y) #Create samples
samples.theta = samples[length(samples[,1]), -c(1:100)] # theta samples, excluding the first 100 values as burnin
samples.f = samples[-length(samples[,1]), -c(1:100)] # Extracting the f samples, excluding the first 100 values as burnin

# Estimate for the posterior marginal for theta
truehist(samples.theta,
         xlab = "theta",
         main = "Histogram of theta samples, Gibbs-MCMC posterior marginal for theta")



```
```{R}
# latent field x_10
f_10 = samples.f[10,]
truehist(f_10, xlab = "x_10", main = "Gibbs-MCMC posterior marginal for x_10")

```

Next we calculate the 95% confidence bounds

```{R}
# latent field x
f.mean = rep(0,T)
f.var = rep(0,T)
conf_interval.upper = rep(0,T)
conf_interval.lower = rep(0,T)
# Calculate the mean and variance
for(t in 1:T) {
  f.mean[t] = mean(samples.f[t,])
  f.var[t] = var(samples.f[t,])
}
# Calculate 95% confidence bounds of smooth effect
zigma = qnorm(0.025)
for(t in 1:T) {
  conf_interval.upper[t] = f.mean[t]+zigma*sqrt(f.var[t])
  conf_interval.lower[t] = f.mean[t]-zigma*sqrt(f.var[t])
}

Ts = seq(T)
plot(Ts,
     y,
     main="Gibbs-MCMC inference for the latent field",
     xlab="t", ylab="value")

lines(Ts, f.mean)
lines(Ts, conf_interval.lower, lty=2)
lines(Ts, conf_interval.upper, lty=2)

```

The histogram of theta samples shows an estimate for $\pi(\theta\mid\mathbf{y})$. The second histogram of $eta_{10}$ samples is an estimate of $\pi(\eta_{10}\mid\mathbf{y})$.

In the Gibbs MCMC plot the data from Gaussian Data are plotted as circles. The linearly interpolated posterior means are shown as a solid line, while the bounds of the 95% confidence interval are plotted as the dotted lines. The Gibbs MCMC is sampled with 10.000 samples and a burnin-period of 100 samples.

## 3

Another method to approximate $\pi(\theta\mid\mathbf{y})$ is using the INLA schemes. In this task the following starting point is used:
$$
\pi(\theta|\mathrm{x}) \propto \frac{\pi(\mathrm{y}|\mathrm{x},\theta)\pi(\mathrm{x}|\theta)\pi(\theta)}{\pi(\mathrm{x}|\theta,\mathrm{y})}
$$
Since the likelihood is Gaussian then also $\pi(\eta \mid \theta, \mathbf{y})$ is Gaussian.

An overview of the distributions are shown below:

- $\pi(\mathrm{y}|\mathrm{x},\theta)=\pi(\mathrm{y}|\mathrm{x})\sim\mathcal{N}(\mathrm{x},\mathrm{I})$
- $\pi(\mathrm{x}|\theta) \sim \mathcal{N}(0,\mathrm{Q}(\theta)^{-1})$
- $\pi(\theta) \sim \textrm{Gamma}(1,1)$
- $\pi(\mathrm{x}|\theta,\mathrm{y})\sim\mathcal{N}(\mathrm{\mu},\mathrm{\Sigma})$

Earlier we found that $\pi(\boldsymbol\eta\mid\theta,\mathbf{y}) \propto \mathcal{N}((\mathbf{Q}+\mathbf{I})^{-1}\mathbf{y},(\mathbf{Q}+\mathbf{I})^{-1})$, therefore we can calculate:
$$
  \begin{aligned}
    \pi(\theta\mid\mathbf{y}) &\propto \frac{\pi(\mathbf{y}\mid\boldsymbol\eta,\theta)\pi(\boldsymbol\eta\mid\theta)\pi(\theta)}{\pi(\boldsymbol\eta\mid\theta,\mathbf{y})} \\
    &\propto \frac{\exp(-\frac{1}{2}(\mathbf{y}-\boldsymbol\eta)^T(\mathbf{y}-\boldsymbol\eta))\theta^{(T-2)/2}\exp(-\frac{1}{2}\boldsymbol\eta^T\mathbf{Q}\boldsymbol\eta)\exp(-\theta)}{|\mathbf{Q}+\mathbf{I}|^{1/2}\exp(-\frac{1}{2}(\boldsymbol\eta-(\mathbf{Q}+\mathbf{I})^{-1}\mathbf{y})^T(\mathbf{Q}+\mathbf{I})(\boldsymbol\eta-(\mathbf{Q}+\mathbf{I})^{-1}\mathbf{y}))} \\
    &= \theta^{(T-2)/2}|\mathbf{Q}+\mathbf{I}|^{-1/2}\exp\bigg(-\theta-\frac{1}{2}\mathbf{y}^T\big(\mathbf{I}-(\mathbf{Q}+\mathbf{I})^{-1}\big)\mathbf{y}\bigg)
  \end{aligned}\quad,
$$


This will be approximated using a grid of value between 0 and 6 and uses 5 to construct an approximation of $\pi(\theta\mid\mathbf{y})$. A function calculating the posterior marginal is shown below:

```{R}
calc_pi_theta_y = function(theta.grid, y) {
  # Calculate pi(theta|y) for each theta in the grid
  pi = rep(0,length(theta.grid)) #Vector to store pi
  T = length(y)
  
  #Iterate and calculate values for pi
  for (i in 1:length(theta.grid)) {
    theta = theta.grid[i]
    deter = det(solve(Q*theta+diag(T))) #Uses same Q as earlier
    pi[i] = theta^(T/2-1) * exp(-theta) * deter^(0.5) * exp(-0.5 * t(y) %*% (diag(T)-solve(Q*theta+diag(T))) %*% y)
  }
  return(pi)
}

theta_grid = seq(from = 0, to = 9, by = 0.1)
pi = calc_pi_theta_y(theta_grid, y) #Calculate values for pi(theta|y)
plot(theta_grid,
     pi,
     xlab = "theta",
     ylab = "pi",
     type = "l",
     main="INLA-inference for hyper posterior",
     )

```

The histogram of using Gibbs-MCMC posterior marginal for theta in task 2 and the plot of the MCMC using INLA-schemes seems to match good.


## 4

We want to implement the next step in the INLA scheme, the approximation of the marginal
posterior for the smooth effect, $\pi(\eta_i\mid y)$.

We have
$$
\begin{aligned}
    \pi(\eta_i\mid\mathbf{y}) &= \int             \pi(\eta_i\mid\mathbf{y},\theta)\pi(\theta\mid\mathbf{y})d\theta \\
    &\approx \sum_{\theta_k\in\boldsymbol\theta_{\text{grid}}}                 \pi(\eta_i\mid\mathbf{y},\theta_k)\pi(\theta_k\mid\mathbf{y}) \Delta
\end{aligned} \quad,
$$

where $\boldsymbol\theta_{\text{grid}}$ is the grid of theta values from task 3, and $\Delta$ is the step size between the grid-values. We assume that $\pi(\eta_i\mid\mathbf{y},\theta)\sim \mathcal{N}([\mathbf{A}\mathbf{y}]_i,\mathbf{A}_{ii})$, because $\pi(\boldsymbol\eta\mid\theta,\mathbf{y}) \propto \mathcal{N}((\mathbf{Q}+\mathbf{I})^{-1}\mathbf{y},(\mathbf{Q}+\mathbf{I})^{-1})$ where $\mathbf{A}=(\mathbf{Q}+\mathbf{I})^{-1}$.

```{R}
calc_eta_i_y_theta = function(eta_i.grid, theta, y) {
  # Calculate pi(eta_10|y,theta_k) 
  T = length(y)
  i = 10
  A = solve(Q*theta + diag(T)) #Same Q as earlier
  mean = (A %*% y)[i]
  var = A[i, i]
  pi = dnorm(eta_i.grid, mean = mean, sd= sqrt(var))
  return (pi) #Vector for each eta_i in the grid
}

calc_pi_eta_i_y = function(theta.grid, eta.grid, y) {
  #Calculate pi(eta_10|y)
  sums = rep(0, length(eta.grid)) # Store approximations
  step = theta.grid[2]-theta.grid[1] # Step size, assumed as diff between the two first elements
  theta_y = calc_pi_theta_y(theta.grid, y) # pi(theta|y) for each theta in the grid
  
  for (i in (1:length(theta.grid))) {
    # Add the terms for theta[i] for each iteration
    sums = sums + calc_eta_i_y_theta(eta.grid, theta.grid[i], y)*theta_y[i]*step
  }
  return (sums)
}

theta_grid = seq(from = 0, to = 9, by = 0.1)
eta.grid = seq(-2, 2, 0.01)
eta_i_y = calc_pi_eta_i_y(theta_grid, eta.grid, y) # Calculate grid


plot(eta.grid,
     eta_i_y,
     ylab = "pi",
     xlab = "eta_10",
     type = "l",
     main = "INLA posterior conditioned on theta"
     )

```

 Comparing the approximation for $\pi(\eta_n\mid \vect{y})$ with the estimation obtained in point 2) via Gibbs sampling, both graphs looks normal with a small positive mean. 


## 5

We want to use the inla() function in R to implement the same model as previous. 

```{R}
library(INLA)

#Initialize values
T = length(y)
Ts = seq(from = 1, to = T, by = 1)
data = data.frame(y = y, t = Ts)

thetahyper = list(theta = list(prior = "log.gamma", param = c(1, 1)))
formula = y ~ f(Ts,
                model = "rw2",
                hyper = thetahyper,
                constr = FALSE) - 1

#Calculate
result_inla = INLA::inla(formula = formula,
                         family = "gaussian",
                         data = data,
                         control.family = list(hyper=list(prec =list(initial=0,fixed=TRUE))))

```
The plot shows the estimated smooth effect using INLA (red line) and the smooth effect calculated in 2) using MCMC (Black line). As one could see, the lines seems to overlap and the estimates are very similar.

```{R}


plot(result_inla$summary.random$Ts$mean,
     xlab="t",
     ylab="y",
     type="l",
     col="red",
     main="R-INLA estimated smooth effect compared to MCMC",
     
     )
lines(Ts, f.mean)

```
The figure below shows the R-INLA estimate of $\pi(\theta\mid y)$ which is also similar to the estimates in task 2 and 3. 

```{R}

# marginal hyperparam
INLAtheta = inla.smarginal(result_inla$marginals.hyperpar[[1]])
plot(INLAtheta,
     type="l",
     main="R-INLA approximation for hyper marginal",
     xlab="theta",
     ylab="density",
     xlim=c(0,6))



```

The figure below shows the R-INLA estimate of $\pi(\eta_{10}\mid y)$, and is also very similar to the estimates in 3 and 4.

```{R}
# marginal eta_10
plot(result_inla$marginals.random$Ts$index.10,
     type="l",
     main="R-INLA posterior for eta_10",
     xlab="eta_10",
     xlim=c(-4,4),
     ylab="density")
```

