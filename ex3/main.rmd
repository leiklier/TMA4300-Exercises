
--- 
title: 'TMA4300 Computer Intensive Statistical Methods Exercise 2, Spring 2021'
output:
  html_document:
    toc: yes
    toc_depth: 2
    toc_float: yes
  pdf_document:
    toc: no
    toc_depth: '2'
date: "`r format(Sys.time(), '%d.%m.%Y')`"
subtitle: 'Group members: Eide, Jonathan and Lima-Eriksen, Leik'
---

\usepackage{amsmath}

\newcommand{\indep}{\perp \!\!\! \perp}
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1} }}
\newcommand{\matr}[1]{\boldsymbol{\mathbf{#1} }}
\newcommand{\Var}{\textrm{Var}}
\newcommand{\E}{\textrm{E}}
\newcommand{\Cov}{\textrm{Cov}}
\newcommand{\Corr}{\textrm{Corr}}

```{r setup, include = FALSE}
library(formatR)
library(ggpubr)
library(ggplot2)
library(coda)
library(cowplot)
library(MASS)
```

# Problem A: Comparing AR(2) parameter estimators using resampling of residuals
In this problem we want to analyze a non-Gaussian time-series of length 
$T = 100$, and compare two different parameter estimators. We consider an AR(2)
model which is specified by the relation

$$
  x_t = \beta_1 x_{t-1} + \beta x_{t-2} + e_t
$$

where $e_t$ are i.i.d.random variables with zero mean and constant variance.

The least sum of squared residuals (LS) and least sum of absolute residuals (LA)
are obtained by minimizing the following loss functions with respect to $\beta$:

$$
\begin{aligned}
  Q_{LS}(\vect{x}) &= \sum_{t=3}^T \left(x-t - \beta_1 x_{t-1} \beta_2 x_{t-2}\right)^2 \\
  Q_{LA}(\vect{x}) &= \sum_{t=3}^T \left| x_t - \beta_1 x_{t-1} - \beta_2 x_{t-2} \right|
\end{aligned}
$$

We denote the minimizers by $\vect{\hat{\beta}}_{LS}$ and $\vect{\hat{\beta}}_{LA}$, and define the estimated residuals to be $\hat{e}_t = x_t - \hat{\beta}_1 x_{t-1} - \hat{\beta}_2 x_{t-2}$ for $t = 3, \dots, T$, and let $\bar{e}$ be the mean of these.then we re-center $\hat{e}_t$ to have mean zero by defining $\hat{\epsilon} = \hat{e}_t - \bar{e}$.

We want to use the residual resampling bootstrap method to evaluate the relative performance of the two
parameter estimators. The estimators are calculated using the function `ARp.beta.est(...)`, and we can use
these estimators together with the function `ARp.resid(...)` to calculate the residuals:
```{R}
n = length(data3A$x)
beta = ARp.beta.est(data3A$x, 2)

e.observed.LS = ARp.resid(data3A$x, beta$LS)
e.observed.LA = ARp.resid(data3A$x, beta$LA)
```

Next, we want to estimate the variance and bias of the two estimators. We will be using $B = 1500$ bootstrap
samples, where each sample is as long as the original data sequence ($T = 100$). To do a resampling, we
initialize values for $x_1$ and $x_2$ by picking a random consecutive subsequence from the data. Then we use
the function `ARp.filter(...)` to generate a new sample based on the bootstrapped residuals. And finally, we
perform a regression on the new time-series to obtain bootstrapped estimates of the different $\vect{\beta}$-s:

```{R}
B = 1500

# Bootstrap the residuals B times
e.bootstrapped = matrix(sample(e.observed.LS, size=B*(n-2), replace=TRUE), nrow=B, ncol=n-2)

# Initialize x0 so that it contains a random sequence
# of two consecutive samples:
indices = sample(99, B, replace=TRUE)
indices.matrix = matrix(c(indices, indices+1), nrow=B, ncol=2)
x0 = matrix(data3A$x[indices.matrix], nrow=B, ncol=2)

```



# Problem B: Permutation test

## 1:
Bilirubin (see http://en.wikipedia.org/wiki/Bilirubin) is a breakdown product of haemoglobin,
which is a principal component of red blood cells. If the liver has suffered degeneration, if the decomposition of haemoglobin is elevated, or if the gall bladder has been destroyed, large amounts of bilirubin can
accumulate in the blood, leading to jaundice. The following data (taken from JÃ¸rgensen (1993)) contain
measurements of the concentration of bilirubin (mg/dL) in blood samples taken from three young men.

We use a boxplot to inspect the data from three different individuals:
```{R}
#Load data
bilirubin <- read.table("bilirubin.txt",header=T)

#Boxplot of data
boxplot(log(meas)~pers,
        data=bilirubin,
        main="Boxplot of Concentration (Logarithmic scale)",
        xlab="Individual",
        ylab="log(meas)"
        )

```

It seems like the levels are higher for p3, and we want to investigate whether the difference is significant.

We fit the linear regression model using $ \textbf{lm} $:

\begin{equation} \label{eq:model_B1}
\ln(Y_{i,j}) = \beta_i + \epsilon_{i,j}, \qquad \textrm{with } i = 1,2,3 \textrm{ and } j = 1, \dots, n_i
\end{equation}

where $n_1=11, n_2=10$ and $n_3=8$, and $\epsilon \sim N(0, \sigma^2).$ 

```{R}
model.reg = lm(log(meas)~pers, data=bilirubin)
summary.lm = summary(model.reg)

summary.lm

```
We want to test the hypotesis that all the individuals bilrubin levels are the same for all individuals: $\beta_1=\beta_2=\beta_3,$. This is tested using the F-test from statistic in summary. The p-value is 0.03946, which means that we can reject the hypothesis that all individuals have the same bilirubin-levels at a sigificance level of 5%.


```{R}
Fval = summary.lm$fstatistic

Fval

```

## 2:

We want to write a function $\textbf{permTest()}$ which generates a permutation of the data between the three individuals, consequently fits the model given in (1) and finally returns the value of the F-statistic for
testing $\beta_1=\beta_2=\beta_3,$.


```{R}
permTest = function(bilirubin) {
  df = data.frame(meas=sample(bilirubin$meas), pers = bilirubin$pers)
  model.reg = lm(log(meas)~pers, data=df)
  Fval = summary(model.reg)$fstatistic["value"]
  return(Fval)
}

#Test the function
permTest(bilirubin)


```

##3:

Next we perform a permutation test between the three individuals using the function $\textbf{permTest()}$ to check whether there is an actual difference in the bilirubin level in the individuals. This is done generating 999 samples of the F-statistic, and comparing this to the original F-statistic, Fval.If the F-statistic is as high or higher than the original F-statistic, there is no difference in the bilirubin levels of the persons.

```{R}
#Create samples
samples = rep(0,999)
for(i in 1:999){
  samples[i] = permTest(bilirubin)
}

# Compute p-value for Fval using samples:
p.val = length(samples[samples>=Fval["value"]])/999

p.val

```
The result is a p-value of 0.036, which means that at a significance level of 5% we reject the hypothesis that the bilirubin levels are the same for all persons.

# Problem C: The EM-algorithm and bootstrapping

## Task 1)

Let $\left\{x_i\right\}_{i=1,\dots,n}$ be i.i.d $\text{Exp}(\lambda_0)$ and $\left\{y_i\right\}_{i=1,\dots,n}$ be i.i.d $\text{Exp}(\lambda_1)$. We assume that we observe neither $x_i$ nor $y_i$ directly, but rather that we observe

$$
  z_i = \max\left(x_i, y_i\right) \quad \forall \quad i = 1, \dots, n
$$
  
and

$$
  u_i = I(x_i \geq y_i) \quad \forall \quad i = 1, \dots, n
$$

where $I(\cdot)$ is the indicator function. This means that we know that we only observe the largest value in the pair $(x_i, y_i)$, and we know whether the observed value is $x_i$ or $y_i$. Based on the observed $(z_i, u_i), i=1,\dots, n$ we will use the EM algorithm to find the maximum likelihood estimates for $(\lamda_0, \lambda_1)$. 

We are interested in finding the log-likelihood function for the complete data $\vect{x} = \begin{pmatrix}x_1 & \dots & x_n\end{pmatrix}^T$ and $\vect{y} = \begin{pmatrix}y_1 & \dots & y_n\end{pmatrix}^T$. This can be derived from the full conditional of the joint of the complete data:

$$
\pi(x_i, y_i | \lambda_0, \lambda_1) = \pi(x_i | \lambda_0) \times \pi(y_i | \lambda_1) = \lambda_0 \lambda_1 \exp\left\{-\left(\lambda_0 x_i + \lambda_1 y_i\right)\right\}
$$
where we have used the fact that $x_i$ and $y_i$ are assumed independent. Then we get

$$
  \pi(\vect{x}, \vect{y} | \lambda_0, \lambda_1) = \prod_{i=1}^n\pi(x_i, y_i | \lambda_0, \lambda_1) = (\lambda_0 \cdot \lambda_1)^n \exp\left\{ -\lambda_0 \sum_{i=1}^n x_i - \lambda_1 \sum_{i=1}^n y_i \right\}
$$
And so the log-likelihood becomes

$$
\begin{aligned}
l(\vect{x}, \vect{y} | \lambda_0, \lambda_1) &= \ln \pi(\vect{x}, \vect{y} | \lambda_0, \lambda_1) \\
&= n(\ln\lambda_0 + \ln\lambda_1) - \lambda_0 \sum_{i=1}^n x_i - \lambda_1 \sum_{i=1}^n y_i
\end{aligned}
$$

The full conditionals of $x_i$ and $y_i$ are found to be

$$
  \pi(x_i | z_i, u_i, \lambda_0^{(t)}, \lambda_1^{(t)}) = \begin{cases}
    \frac{\lambda_0^{(t)} \exp\left\{-\lambda_0^{(t)}x_i\right\}}{ 1-\exp\left\{-\lambda_0^{(t)}z_i\right\}}, \quad &u_i = 0,\\
    z_i, \quad &u_i = 1.
  \end{cases}
$$
$$
  \pi(y_i | z_i, u_i, \lambda_0^{(t)}, \lambda_1^{(t)}) = \begin{cases}
    z_i, \quad &u_i = 0, \\
    \frac{\lambda_1^{(t)} \exp\left\{-\lambda_1^{(t)}y_i\right\}}{ 1-\exp\left\{-\lambda_1^{(t)}z_i\right\}}, \quad &u_i = 1.
  \end{cases}
$$

The EM algorithm is based on maximizing $\E[l(\vect{x}, \vect{y} | \lambda_0, \lambda_1)]$ assuming that $\vect{x}$ and $\vect{y}$ comes from the distributions for $\vect{z}$ and $\vect{u}$. We also have to make some initial guess on the intensities $\lambda_0^{(t)}$ and $\lambda_1^{(t)}$. Then we get

$$
  \begin{aligned}
  \E[l(\vect{x}, \vect{y} | \lambda_0, \lambda_1) | \vect{z}, \vect{u}, \lambda_0^{(t)}, \lambda_1^{(t)}] &= n \cdot (\ln \lambda_0 + \ln \lambda_1)\\
  &- \lambda_0 \sum_{i=1}^n \E[x_i | z_i, u_i, \lambda_0^{(t)}, \lambda_1^{(t)}] \\
  &- \lambda_1 \sum_{i=1}^n \E[y_i | z_i, u_i, \lambda_1^{(t)}, \lambda_1^{(t)}]
  \end{aligned}
$$

The expectations mentioned in the above equations are found as follows:

$$
\begin{aligned}
  \E[x_i | z_i, u_i, \lambda_0^{(t)}, \lambda_1^{(t)}] &= u_i z_i + (1 - u_i) \int_0^{z_i} x_i \frac{\lambda_0^{(t)} \exp\left\{-\lambda_0^{(t)} x_i\right\}}{1 - \exp\left\{-\lambda_0^{(t)} z_i\right\}} dx_i \\
  &= u_i z_i + (1 - u_i) \left( \frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp\left\{\lambda_0^{(t)z_i}\right\} - 1}\right)
\end{aligned}
$$

$$
\begin{aligned}
  \E[y_i | z_i, u_i, \lambda_0^{(t)}, \lambda_1^{(t)}] &= (1 - u_i) z_i + u_i \int_0^{z_i} x_i \frac{\lambda_1^{(t)} \exp\left\{-\lambda_1^{(t)} y_i\right\}}{1 - \exp\left\{-\lambda_1^{(t)} z_i\right\}} dx_i \\
  &= (1 - u_i) z_i + u_i \left( \frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp\left\{\lambda_0^{(t)z_i}\right\} - 1}\right)
\end{aligned}
$$

In order to maximize the expectation of $l(\vect{x}, \vect{y} | \lambda_0, \lambda_1)$, let's first define the function

$$
  \begin{aligned}
  Q(\lambda_0, \lambda_1 | \lambda_0^{(t)}, \lambda_1^{(t)}) &= \E[l(\vect{x}, \vect{y} | \lambda_0, \lambda_1) | \vect{z}, \vect{u}, \lambda_0^{(t)}, \lambda_1^{(t)}] \\
  &= n \cdot (\ln \lambda_0 + \ln \lambda_1) \\
  &- \lambda_0 \sum_{i=1}^n \left[u_i z_i + (1 - u_i) \left(\frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp\left\{\lambda_0^{(t)} z_i\right\}}\right)\right] \\
  &- \lambda_1 \sum_{i=1}^n \left[ (1 - u_i) z_i + u_i \left(\frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp\left\{\lambda_1^{(t)} z_i\right\} - 1}\right)
  \right]\end{aligned}
$$

## Task 2)

The function $Q(\cdot)$ attains a maximum, and it is analytically tractable by considering

$$
  \frac{\partial Q}{\partial \lambda_0} = 0 \quad \text{and} \quad \frac{\partial Q}{\partial \lambda_1} = 0
$$
Solving for $\lambda_0$ and $\lambda_1$ we end up with

$$
  \begin{aligned}
  \lambda_0 &= \frac{n}{\sum_{i=1}^n \left[(1 - u_i) z_i + u_i \left(\frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp\left\{\lambda_0^{(t)}z_i\right\} - 1}\right)\right] } \\
  \lambda_1 &= \frac{n}{\sum_{i=1}^n \left[u_i z_i + (1 - u_i) \left(\frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp\left\{\lambda_1^{(t)}z_i\right\} - 1}\right)\right] } 
  \end{aligned}
$$

The above values for $(\lambda_0, \lambda_1)$ can be used as $\left(\lambda_0^{(t+1)}, \lambda_1^{(t + 1)}\right)$ in the recursion $\left(\lambda_0^{(t)}, \lambda_1^{(t)}\right) \mapsto \left(\lambda_0^{(t+1)}, \lambda_1^{(t + 1)}\right)$. Below we have implemented the recursion in order to visualise the convergence plots for the estimates.

```{R}
u = read.delim("./u.txt")[[1]]
z = read.delim("./z.txt")[[1]]
n = length(u)


num_iterations = 20

lambda_0 = c(1, vector(length=num_iterations-1))
lambda_1 = c(18, vector(length=num_iterations-1))

for(i in 2:num_iterations) {
  lambda_0[i] = n/sum(u*z + (1 - u)*(1/lambda_0[i-1] - z/(exp(lambda_0[i-1]*z) - 1)))
  lambda_1[i] = n/sum((1 - u)*z + u*(1/lambda_1[i - 1] - z/(exp(lambda_1[i-1] * z) - 1)))
}

df_lambda_0 = data.frame(lambda_0=abs(diff(lambda_0)), iter=1:(num_iterations-1))
df_lambda_1 = data.frame(lambda_1=abs(diff(lambda_1)), iter=1:(num_iterations-1))

ggplot() + geom_line(data=df_lambda_0, aes(x=iter, y=lambda_0, color="lambda_0")) + geom_line(data=df_lambda_1, aes(x=iter, y=lambda_1, color="lambda_1")) + xlab("Iteration") + ylab("|lambda^(t+1) - lambda^t|")
```

```{R}
cat("lambda_0 after 20 iterations:", lambda_0[20], "\n")
cat("lambda_1 after 20 iterations:", lambda_1[20], "\n")
```


From the above plot we see that the EM algorithm makes the maximum likelihood estimates $\hat{\lambda_0}$ and $\hat{\lambda_1}$ for $\lambda_0$ and $\lambda_1$ respectively converge very fast. After 20 iterations, we find $\hat{\lambda_0} = 3.46$ and $\hat{\lambda_1} = 9.33$.

## 3:

Now we want to investigate the bias and variance of the estimator defined by the EM-algorithm. This we can do by using the bootstrap algorithm. In pseudocode, we want to do the following:

for $b$ in $(1, 2, \dots, B)$ do

1. $u_{b}^* \leftarrow$ sample with replacement from $u$
2. $z_{b}^* \leftarrow$ sample with replacement from $z$
3.  $(\lambda_{0, b}^{*}, \lambda_{1, b}^{*}) \leftarrow$ run EM algorithm with $u_{b}^*$ and $z_{b}^*$
4. store $(\lambda_{0, b}^{*}, \lambda_{1, b}^{*})$ as a single bootstrap estimate.

for b in (1, 2, ..., B) do:
  1. Sample $z_b^*$ with replacement from $z$
  2. Sample $u_b^*$ with replacement from $u$
  3. Obtain a bootstrap estimate $(\lambda_{0, b}^{*}, \lambda_{1, b}^{*})$ by using the EM-algorithm with $z_b^*$ and $u_b^*$. 
  4. Store the estimate $(\lambda_{0, b}^{*}, \lambda_{1, b}^{*})$ 
end for

We also need to define a stopping criteria for the EM-algorithm. In our case, we will be using the two-norm with tolerance $10^{-10}$.

```{R}
B = 20000
tolerance = 10^(-10)
EM_algorithm = function(u, z, tolerance) {
  lambda = runif(2, 1, 10)
  lambda_prev = c(0, 0)
  
  while(sqrt(sum((lambda-lambda_prev)^2)) > tolerance){
    lambda_prev = lambda
    lambda[1] = n / sum(u*z + (1-u)*(1/lambda[1] - z/(exp(lambda[1]*z) - 1)))
    lambda[2] = n / sum((1-u)*z + u*(1/lambda[2] - z/(exp(lambda[2]*z) - 1)))
  }
  
  return(lambda)
}

lambdas = matrix(0, nrow=B, ncol=2)
for(b in 1:B) {
  i = sample(1:n, replace=TRUE)
  z_b = z[i]
  u_b = u[i]
  lambdas[b,] = EM_algorithm(u_b, z_b, tolerance)
}


# Plots

par(mfrow=c(1,2))
truehist(lambdas[,1], xlab="lambda_0")
abline(v=lambda_0[20], col="red")
truehist(lambdas[,2], xlab="lambda_1")
abline(v=lambda_1[20], col="red", lwd=2)
```

It seems that the estimators are unbiased simply by looking at the histograms above. Here we plotted the EM estimates as red lines. We can verify this by calculating the bias as has been done below:

```{R}
lambda_0_bs_mean = mean(lambdas[,1])
lambda_1_bs_mean = mean(lambdas[,2])
lambda_0_bias = lambda_0_bs_mean - lambda_0[20]
lambda_1_bias = lambda_1_bs_mean - lambda_1[20]
cat("Bias lambda_0: ", lambda_0_bias, "\n")
cat("Bias lambda_1: ", lambda_1_bias, "\n")
```

However, the standard deviation is quite large, and this would probably contribute more to the estimation error than the bias. We calculate the standard deviations below:

```{R}
sd = sqrt(diag(cov(lambdas)))
cat("SD lambda_0: ", sd[1], "\n")
cat("SD lambda_1: ", sd[2], "\n")
```

Also, the correlation between the estimates are as follows:

```{R}
corr = cor(lambdas)[1,2]
cat("Correlation: ", corr, "\n")
```

Because of this, we would prefer using the maximum likelihood estimate. Also, since we have a small correlation it indicates that our algorithm is correctly implemented, since $x_i$ and $y_i$ are independent.

## Task 4)

We now want to find an analytical formula for $\pi(z_i u_i | \lambda_0, \lambda_1)$. We start by calculating the cumulative distribution for $z_i$ when $u_i = 1$, implying $z_i = x_i$:

$$
  \begin{aligned}
  F(z_i | u_i = 1, \lambda_0, \lambda_1) &= \int_0^{x_i} \int_0^{x_i} \pi(x_i | \lambda_0) \times \pi(y_i | \lambda_1) \text{d} y_i \text{d} x_i \\
  &= \int_0^{z_i} \lambda_0 e^{-\lambda_0 x_i} (1 - e^{-\lambda_1 y_i}) \text{d} x_i
  \end{aligned}
$$

Then we can obtain the pdf

$$
  \pi(z_i | u_i = 1, \lambda_0, \lambda_1) = \lambda_0 e^{-\lambda_0 z_i} (1 - e^{-\lambda_1 z_i})
$$

We can do the same for $u_i = 0$, and we end up with

$$
  \pi(z_i | u_i = 0, \lambda_0, \lambda_1 ) = \lambda_1 e^{-\lambda_1 z_i}(1 - e^{-\lambda_0 z_i})
$$

Combining these expressions, we can write 

$$
  \pi(z_i, u_i | \lambda_0, \lambda_1) = \begin{cases}
    \lambda_1 e^{-\lambda_1 z_i}\left(1 - e^{-\lambda_0 z_i}\right), \quad &u_i = 0\\
    \lambda_0 e^{-\lambda_0 z_i} (1 - e^{-\lambda_1 z_i}), \quad &u_i = 1
  \end{cases}
$$

Define $n_0 = \sum_{i=0}^n I(u_i = 0)$ and $n_0 = \sum_{i=0}^n I(u_i = 1)$. Then we can easily find the loglikelihood as

$$
\begin{aligned}
  l(\lambda_0, \lambda_1 | \vect{z}, \vect{u}) &= \ln \pi(\vect{z}, \vect{u} | \lambda_0, \lambda_1) \\
  &= \ln \left(\prod_{i=1}^n \pi(z_i, u_i | \lambda_0, \lambda_1)\right) \\
  &= \sum_{i=1}^n \ln \left( \pi(z_i, u_i | \lambda_0, \lambda_1\right)) \\
  &= n_0 \ln \lambda_1 + n_1 \lambda_0\\
  &+ \underset{u_i = 1}{\sum_{i=1}^n} \left( \ln(1-e^{-\lambda_0 z_i})- \lambda_1 z_i \right) + \underset{u_i = 1}{\sum_{i=1}^n} \left( \ln(1-e^{-\lambda_1 z_i})- \lambda_0 z_i \right)
\end{aligned}
$$

We want to find the maximum likelihood estimator for $\lambda_0$ and $\lambda_1$, and this is found through finding the maximum of the loglikelihood. We then have to find extremal values of this function:

$$
\begin{aligned}
  0 &= \frac{\partial l(\lambda_0, \lambda_1 | \vect{z}, \vect{u})}{\partial \lambda_0} = \frac{n_1}{\lambda_0} - \sum_{i=1}^n z_i + \underset{u_i = 0}{\sum_{i=1}^n} \frac{z_i e^{\lambda_0 z_i}}{e^{\lambda_0 z_i} - 1} \\
  0 &= \frac{\partial l(\lambda_0, \lambda_1 | \vect{z}, \vect{u})}{\partial \lambda_1} = \frac{n_0}{\lambda_1} - \sum_{i=1}^n z_i + \underset{u_i = 1}{\sum_{i=1}^n} \frac{z_i e^{\lambda_1 z_i}}{e^{\lambda_1 z_i} - 1}
\end{aligned}
$$

The hessian is as follows:

$$
  \nabla^2 l(\lambda_0, \lambda_1 | \vect{z}, \vect{u}) = \begin{pmatrix}
    - \frac{n_1}{\lambda_0^2} - \underset{u_i = 0}{\sum_{i=1}^n} \frac{z_i^2 e^{\lambda_0 z_i}}{\left(e^{\lambda_0 z_i}-1\right)^2} & 0 \\
    0 & - \frac{n_0}{\lambda_1^2} - \underset{u_i = 1}{\sum_{i=1}^n} \frac{z_i^2 e^{\lambda_1 z_i}}{\left(e^{\lambda_1 z_i}-1\right)^2}
  \end{pmatrix}
$$
Since the hessian is negative definite for all $(\lambda_0, \lambda_1)$, there exists a unique maximum which is unique. We will be using R's built-in optimization function `optim` in order to find the maximum:

```{R}
loglikelihood = function(lambdas) {
  idx_u_0 = which(u == 0)
  idx_u_1 = which(u == 1)
  n0 = length(idx_u_0)
  n1 = length(idx_u_1)
  ll = n0 * log(lambdas[2])+n1*log(lambdas[1]) +
    sum(log(1-exp(-lambdas[1]*z[idx_u_0])) - lambdas[2]*z[idx_u_0]) +
    sum(log(1-exp(-lambdas[2]*z[idx_u_1])) - lambdas[1]*z[idx_u_1])
}

mle_lambdas = optim(par=c(1,1), fn=loglikelihood, control=list(fnscale=-1, maxit=1000, reltol=1e-15))$par
mle_lambdas
```

This matches the estimates from the EM-algorithm. In the general case, EM convergence can be slow compared to using classical optimization. However, classical optimization is also more difficult to implement since it requires a lot of calculations beforehand.