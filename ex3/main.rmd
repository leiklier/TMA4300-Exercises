
--- 
title: 'TMA4300 Computer Intensive Statistical Methods Exercise 2, Spring 2021'
output:
  html_document:
    toc: yes
    toc_depth: 2
    toc_float: yes
  pdf_document:
    toc: no
    toc_depth: '2'
date: "`r format(Sys.time(), '%d.%m.%Y')`"
subtitle: 'Group members: Eide, Jonathan and Lima-Eriksen, Leik'
---

\usepackage{amsmath}

\newcommand{\indep}{\perp \!\!\! \perp}
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1} }}
\newcommand{\matr}[1]{\boldsymbol{\mathbf{#1} }}
\newcommand{\Var}{\textrm{Var}}
\newcommand{\E}{\textrm{E}}
\newcommand{\Cov}{\textrm{Cov}}
\newcommand{\Corr}{\textrm{Corr}}

```{r setup, include = FALSE}
library(formatR)
library(ggpubr)
library(ggplot2)
library(coda)
library(cowplot)
```

# Problem A: Comparing AR(2) parameter estimators using resampling of residuals
In this problem we want to analyze a non-Gaussian time-series of length 
$T = 100$, and compare two different parameter estimators. We consider an AR(2)
model which is specified by the relation

$$
  x_t = \beta_1 x_{t-1} + \beta x_{t-2} + e_t
$$

where $e_t$ are i.i.d.random variables with zero mean and constant variance.

The least sum of squared residuals (LS) and least sum of absolute residuals (LA)
are obtained by minimizing the following loss functions with respect to $\beta$:

$$
\begin{aligned}
  Q_{LS}(\vect{x}) &= \sum_{t=3}^T \left(x-t - \beta_1 x_{t-1} \beta_2 x_{t-2}\right)^2 \\
  Q_{LA}(\vect{x}) &= \sum_{t=3}^T \left| x_t - \beta_1 x_{t-1} - \beta_2 x_{t-2} \right|
\end{aligned}
$$

We denote the minimizers by $\vect{\hat{\beta}}_{LS}$ and $\vect{\hat{\beta}}_{LA}$, and define the estimated residuals to be $\hat{e}_t = x_t - \hat{\beta}_1 x_{t-1} - \hat{\beta}_2 x_{t-2}$ for $t = 3, \dots, T$, and let $\bar{e}$ be the mean of these.then we re-center $\hat{e}_t$ to have mean zero by defining $\hat{\epsilon} = \hat{e}_t - \bar{e}$.

We want to use the residual resampling bootstrap method to evaluate the relative performance of the two
parameter estimators. The estimators are calculated using the function `ARp.beta.est(...)`, and we can use
these estimators together with the function `ARp.resid(...)` to calculate the residuals:


```{r, echo = F, eval = T}
#Load data
source("probAdata.R")
source("probAhelp.R")

```

```{R}

n = length(data3A$x)
beta = ARp.beta.est(data3A$x, 2)
x = data3A$x

e.observed.LS = ARp.resid(data3A$x, beta$LS)
e.observed.LA = ARp.resid(data3A$x, beta$LA)
```

Next, we want to estimate the variance and bias of the two estimators. We will be using $B = 1500$ bootstrap
samples, where each sample is as long as the original data sequence ($T = 100$). To do a resampling, we
initialize values for $x_1$ and $x_2$ by picking a random consecutive subsequence from the data. Then we use
the function `ARp.filter(...)` to generate a new sample based on the bootstrapped residuals. And finally, we
perform a regression on the new time-series to obtain bootstrapped estimates of the different $\vect{\beta}$-s:

```{R}
B = 1500

# Bootstrap the residuals B times
e.bootstrapped = matrix(sample(e.observed.LS, size=B*(n-2), replace=TRUE), nrow=B, ncol=n-2)

# Initialize x0 so that it contains a random sequence
# of two consecutive samples:
indices = sample(99, B, replace=TRUE)
indices.matrix = matrix(c(indices, indices+1), nrow=B, ncol=2)
x0 = matrix(data3A$x[indices.matrix], nrow=B, ncol=2)

beta_LS = beta[[1]]
beta_LA = beta[[2]]
T = length(x)

# Matrices to store the bootstrap samples
bootstrap.beta_LS = matrix(0, nrow = length(beta_LS), ncol = B)
bootstrap.beta_LA = matrix(0, nrow = length(beta_LA), ncol = B)

for (i in (1:B)) {
  # Pick random consecutive sequence x0 from the data
  random_index = sample(1:99, 1)
  x0 = c(x[random_index], x[random_index+1])

  # Reorder data
  x0 = rev(x0)
  resample_residual.LS = sample(e.observed.LS, size = T, replace = TRUE)    
  resample_residual.LA = sample(e.observed.LA, size = T, replace = TRUE)
  
  #Calc new sequence of the data
  x.bootstrap.LS = ARp.filter(x0, beta_LS, resample_residual.LS)[3:(T+2)]
  x.bootstrap.LA = ARp.filter(x0, beta_LA, resample_residual.LA)[3:(T+2)]
  
  #Estimate betas
  beta_1 = ARp.beta.est(x.bootstrap.LS, 2)
  beta_2 = ARp.beta.est(x.bootstrap.LA, 2)

  #Save data
  bootstrap.beta_LS[,i] = beta_1[[1]]
  bootstrap.beta_LA[,i] = beta_2[[2]]
  
}

#Calculate bias
beta_LS.mean = c(mean(bootstrap.beta_LS[1,]), mean(bootstrap.beta_LS[2,]))
beta_LA.mean = c(mean(bootstrap.beta_LA[1,]), mean(bootstrap.beta_LA[2,]))

beta_LS.bias = beta_LS.mean - beta_LS
beta_LA.bias = beta_LA.mean - beta_LA

#Calculate variances
beta_LS.var = c(var(bootstrap.beta_LS[1,]), var(bootstrap.beta_LS[2,]))
beta_LA.var = c(var(bootstrap.beta_LA[1,]), var(bootstrap.beta_LA[2,]))

#Print results
cat("Bias and variance of LS and LA estimators:\n")
cat("Beta LS bias: ", beta_LS.bias, "\n")
cat("Beta LA bias: ", beta_LA.bias, "\n")
cat("Beta LS variance: ", beta_LS.var, "\n")
cat("Beta LA variance: ", beta_LA.var, "\n")

```

The LS estimator is optimal for Gaussian AR(p) processes, however from the results we see that LA has a lower variance and bias than LS. LA is in this case better than LS because the data is not Gaussian.

# Problem B: Permutation test

## 1:
Bilirubin (see http://en.wikipedia.org/wiki/Bilirubin) is a breakdown product of haemoglobin,
which is a principal component of red blood cells. If the liver has suffered degeneration, if the decomposition of haemoglobin is elevated, or if the gall bladder has been destroyed, large amounts of bilirubin can
accumulate in the blood, leading to jaundice. The following data (taken from JÃ¸rgensen (1993)) contain
measurements of the concentration of bilirubin (mg/dL) in blood samples taken from three young men.

We use a boxplot to inspect the data from three different individuals:
```{R}
#Load data
bilirubin <- read.table("bilirubin.txt",header=T)

#Boxplot of data
boxplot(log(meas)~pers,
        data=bilirubin,
        main="Boxplot of Concentration (Logarithmic scale)",
        xlab="Individual",
        ylab="log(meas)"
        )

```

It seems like the levels are higher for p3, and we want to investigate whether the difference is significant.

We fit the linear regression model using $ \textbf{lm} $:

\begin{equation} \label{eq:model_B1}
\ln(Y_{i,j}) = \beta_i + \epsilon_{i,j}, \qquad \textrm{with } i = 1,2,3 \textrm{ and } j = 1, \dots, n_i
\end{equation}

where $n_1=11, n_2=10$ and $n_3=8$, and $\epsilon \sim N(0, \sigma^2).$ 

```{R}
model.reg = lm(log(meas)~pers, data=bilirubin)
summary.lm = summary(model.reg)

summary.lm

```
We want to test the hypotesis that all the individuals bilrubin levels are the same for all individuals: $\beta_1=\beta_2=\beta_3,$. This is tested using the F-test from statistic in summary. The p-value is 0.03946, which means that we can reject the hypothesis that all individuals have the same bilirubin-levels at a sigificance level of 5%.


```{R}
Fval = summary.lm$fstatistic

Fval

```

## 2:

We want to write a function $\textbf{permTest()}$ which generates a permutation of the data between the three individuals, consequently fits the model given in (1) and finally returns the value of the F-statistic for
testing $\beta_1=\beta_2=\beta_3,$.


```{R}
permTest = function(bilirubin) {
  df = data.frame(meas=sample(bilirubin$meas), pers = bilirubin$pers)
  model.reg = lm(log(meas)~pers, data=df)
  Fval = summary(model.reg)$fstatistic["value"]
  return(Fval)
}

#Test the function
permTest(bilirubin)


```

##3:

Next we perform a permutation test between the three individuals using the function $\textbf{permTest()}$ to check whether there is an actual difference in the bilirubin level in the individuals. This is done generating 999 samples of the F-statistic, and comparing this to the original F-statistic, Fval.If the F-statistic is as high or higher than the original F-statistic, there is no difference in the bilirubin levels of the persons.

```{R}
#Create samples
samples = rep(0,999)
for(i in 1:999){
  samples[i] = permTest(bilirubin)
}

# Compute p-value for Fval using samples:
p.val = length(samples[samples>=Fval["value"]])/999

p.val

```
The result is a p-value of 0.036, which means that at a significance level of 5% we reject the hypothesis that the bilirubin levels are the same for all persons.

# Problem C: The EM-algorithm and bootstrapping

Let $\left\{x_i\right\}_{i=1,\dots,n}$ be i.i.d $\text{Exp}(\lambda_0)$ and $\left\{y_i\right\}_{i=1,\dots,n}$ be i.i.d $\text{Exp}(\lambda_1)$. We assume that we observe neither $x_i$ nor $y_i$ directly, but rather that we observe

$$
  z_i = \max\left(x_i, y_i\right) \quad \forall \quad i = 1, \dots, n
$$
  
and

$$
  u_i = I(x_i \geq y_i) \quad \forall \quad i = 1, \dots, n
$$

where $I(\cdot)$ is the indicator function. This means that we know that we only observe the largest value in the pair $(x_i, y_i)$, and we know whether the observed value is $x_i$ or $y_i$. Based on the observed $(z_i, u_i), i=1,\dots, n$ we will use the EM algorithm to find the maximum likelihood estimates for $(\lamda_0, \lambda_1)$. 

We are interested in finding the log-likelihood function for the complete data $\vect{x} = \begin{pmatrix}x_1 & \dots & x_n\end{pmatrix}^T$ and $\vect{y} = \begin{pmatrix}y_1 & \dots & y_n\end{pmatrix}^T$. This can be derived from the full conditional of the joint of the complete data:

$$
\pi(x_i, y_i | \lambda_0, \lambda_1) = \pi(x_i | \lambda_0) \times \pi(y_i | \lambda_1) = \lambda_0 \lambda_1 \exp\left\{-\left(\lambda_0 x_i + \lambda_1 y_i\right)\right\}
$$
where we have used the fact that $x_i$ and $y_i$ are assumed independent. Then we get

$$
  \pi(\vect{x}, \vect{y} | \lambda_0, \lambda_1) = \prod_{i=1}^n\pi(x_i, y_i | \lambda_0, \lambda_1) = (\lambda_0 \cdot \lambda_1)^n \exp\left\{ -\lambda_0 \sum_{i=1}^n x_i - \lambda_1 \sum_{i=1}^n y_i \right\}
$$
And so the log-likelihood becomes

$$
\begin{aligned}
l(\vect{x}, \vect{y}) &= \ln \pi(\vect{x}, \vect{y} | \lambda_0, \lambda_1) \\
&= n(\ln\lambda_0 + \ln\lambda_1) - \lambda_0 \sum_{i=1}^n x_i - \lambda_1 \sum_{i=1}^n y_i
\end{aligned}
$$

The full conditionals of $x_i$ and $y_i$ are found to be

$$
  \pi(x_i | z_i, u_i, \lambda_0^{(t)}, \lambda_1^{(t)}) = \begin{cases}
    \frac{\lambda_0^{(t)} \exp\left\{-\lambda_0^{(t)}x_i\right\}}{ 1-\exp\left\{-\lambda_0^{(t)}z_i\right\}}, \quad &u_i = 0,\\
    z_i, \quad &u_i = 1.
  \end{cases}
$$
$$
  \pi(y_i | z_i, u_i, \lambda_0^{(t)}, \lambda_1^{(t)}) = \begin{cases}
    z_i, \quad &u_i = 0, \\
    \frac{\lambda_1^{(t)} \exp\left\{-\lambda_1^{(t)}y_i\right\}}{ 1-\exp\left\{-\lambda_1^{(t)}z_i\right\}}, \quad &u_i = 1.
  \end{cases}
$$

The EM algorithm is based on maximizing $\E[(\vect{x}, \vect{y})]$