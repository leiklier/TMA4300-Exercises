
--- 
title: 'TMA4300 Computer Intensive Statistical Methods Exercise 1, Spring 2021'
output:
  pdf_document:
    toc: no
    toc_depth: '2'
  html_document:
    toc: yes
    toc_depth: 2
    toc_float: yes
date: "`r format(Sys.time(), '%d.%m.%Y')`"
subtitle: 'Group members: Eide, Jonathan and Lima-Eriksen, Leik'
---

# Problem A

## Part 1 - the exponential distribution

The exponential distribution with rate parameter $\lambda$ has CDF $F(x) = 1 - e^{-\lambda x}$. Sampling from this distribution is equivalent to sampling uniformly on the inverse CDF, i.e:

$$
\begin{aligned}
  u \sim U[0,1] \\
  f(x) = F^{-1}(u) = -\frac{1}{\lambda}\log{u}
\end{aligned}
$$
This is implemented in R the following way:
```{R, echo = T}
  sample_exponential = function(n, lambda) {
    uniforms = runif(n)
    return(exponentials = -1/lambda * log(uniforms))
  }
```

We then want to make sure that our function works correctly. We start off by checking that the sample mean matches the expected value $E[x] = \frac{1}{\lambda}$, and that the sample variance matches the variance $Var[x] = \frac{1}{\lambda^2}$:

```{R, echo = T}
  n = 100000
  lambda = 3.2
  
  x = sample_exponential(n, lambda)
  cat('Theoretical mean:', 1/lambda, '\n')
  cat('Sample mean:', mean(x), '\n----\n')
  cat('Theoretical variance:', 1/lambda^2, '\n')
  cat('Sample variance:', var(x), '\n')
```

We see that there is a good match between the theoretical and obtained values, which indicates that the function works as expected. Of course they would not be exactly equal because we only obtain a finite number of samples.

We then proceed to verify that the samples ensemble the same distribution as the exponential distribution. A good way to visualize this would be to plot a histogram of the samples, and then superimpose the analytic function $f(x) = \lambda e^{-\lambda x}$. This is shown in the figure below:

```{R exp_pdf}
  library(ggplot2)
  df = data.frame(theo=seq(0,max(x),length.out=n), x=x)
  ggplot(df, aes(x=x)) +
    geom_histogram(aes(y = ..density.., color = "Sampled"), binwidth=0.01) +
    stat_function(fun=dexp,geom = "line",size=0.7,args=(mean=lambda),aes(color="Analytical")) +
    ggtitle("Exponential pdf - Sampled vs Analytical") +
    labs(x='x', y='density') +
    xlim(0, 1.5)
```

From \@ref{exp_pdf} we see that the samples follow the exponential distribution perfectly. We thereby conclude that the function samples as expected.

## Part 2

We then want to create a function for sampling from the pdf $g(x)$ as shown below. 

$$
g(x) = \begin{cases} 
  c x^{\alpha - 1}, \quad 0 < x < 1, \\
  c e^{-x}, \quad 1 \leq x, \\
  0, \quad otherwise
\end{cases}
$$

First, note that the normalizing constant $c$ can be found by using the property $\int_{-\infty}^{\infty} g(x) dx = 1$:
$$
  \begin{aligned}
    1 &= \int_0^1 c x^{\alpha - 1} dx + \int_1^{\infty} c e^{-x} dx \\
    &= \frac{c}{\alpha} + \frac{c}{e} \\
    &\Rightarrow c  = \frac{\alpha e}{\alpha + e}
  \end{aligned}
$$
Our aim is then to find an analytic expression for the inverse CDF, so that we can sample uniformly from it to generate samples from $g(x)$ in the same way as we did in Part 1. We start off by first finding the CDF $G(X) = P(X \leq x) = \int_{-\infty}^{x} g(x) dx$:

$$
  \begin{aligned}
  G(x) &= \begin{cases}
    \int_0^xc x^{\alpha - 1} dx ,  \quad x \in (0, 1) \\
    \int_0^x c x^{\alpha - 1}dx + \int_1^xc e^{-x} dx,  \quad x \in [1, \infty)
  \end{cases}\\
  &= \begin{cases}
    \frac{c}{\alpha} x^\alpha,  \quad x \in (0, 1) \\
    \frac{c}{\alpha} +c (e^{-1} - e^{-x}),  \quad x \in [1, \infty)
  \end{cases}
  \end{aligned}
$$

Then the inverse CDF, $G^{-1}(x)$ is found by solving $G^{-1}(G(x)) = x$. The definition limits for the inverse function is obviously different, and is found by evaluating $G^{-1}(x) = 1$, since this is the $x$-value for which the analytic expression changes for the CDF.

$$
  G^{-1}(x) = \begin{cases}
    (\frac{\alpha}{c} x)^{1/\alpha}, \quad x \in (0, \frac{c}{\alpha}), \\
    \ln{\frac{c}{1 - x}} \quad \forall \quad x \in [\frac{c}{\alpha}, 1), \\
    0, \text{ otherwise.}
  \end{cases}
$$


To sample from $g(x)$ is then equivalent to generate $u \sim U[0,1]$, and evaluate $G^{-1}(u)$. Below the `density_g` function allows us to evaluate the density analytically for given values of $x$. Furthermore, `sample_g` is used to sample from $g$ using the inverse CDF as previously specified.

```{R, echo = T, eval = T}
  density_g = function(x, alpha) {
    c = alpha*exp(1)/(alpha + exp(1))
    density = vector(length = length(x))
    density[x < 1.] = c*x[x<1.]^(alpha-1)
    density[x >= 1.] = c*exp(-x[x>=1.])
    return(as.double(density))
  }


  sample_g = function(n, alpha) {
    c = (alpha * exp(1)) / (alpha + exp(1))
    u = runif(n)
    samples = vector(length=length(u))
    samples[u < c/alpha] = (alpha/c*u[u < c/alpha])^(1/alpha)
    samples[u >= c/alpha] = log(c / (1 - u[u >= c/alpha]))
    return(samples)
  }
```

We then want to compare the expected value and variance with the empirical mean and variance respectively. The moments can be calculated as follows:

$$
  E[X] = \int_0^\infty x g(x) dx = \frac{c}{\alpha + 1} + 2 \frac{c}{e}
$$

$$
  Var[X] = E[X^2] -E[X]^2 = \int_0^\infty x^2g(x)dx - E[X]^2 = \frac{c}{\alpha + 2} + 5\frac{c}{e} - (\frac{c}{\alpha + 1} + 2 \frac{c}{e})^2
$$
For $n=1\text{E+5}$ samples with $\alpha = 0.7$, the empirical and theoretical mean and variances are respectively:

```{R, echo = T, eval = T}
  n=100000
  alpha = 0.7
  x = sample_g(n, alpha)
  cat("Empirical mean:", mean(x), "\n")
  c = (alpha * exp(1)) / (alpha + exp(1))
  cat("Theoretical mean:", c/(alpha+1) + 2 * c / exp(1), "\n")
  cat("----\n")
  cat("Empirical variance:", var(x), "\n")
  cat("Theoretical variance:", c/(alpha+2) + 5*c/exp(1) - (c/(alpha+1) + 2*c/exp(1))^2)
```

Lastly, we create a histogram of the samples with the analytic density superimposed. It seems that they coincides well. 
```{R}
  library(ggplot2)
  df <- data.frame(theo=seq(0,max(x),length.out=n), x=x)
  ggplot(df, aes(x = x)) +
    geom_histogram(aes(y = ..density.., color = "Sampled"), binwidth=0.01) +
    stat_function(fun=density_g,geom = "line",size=0.5,args=(mean=alpha),aes(color="Analytical")) + 
    xlim(0, 2) + 
    ggtitle("Piecewise defined pdf - Sampled vs Analytical") + labs(x='x', y='density')
```

## Part 3 - The Box-Muller Algorithm

In order to generate $n$ independent samples from the standard normal distribution, we can use the Box-Muller algorithm. The algorithm makes it possible to generate two iid. samples from $N(0,1)$ according to $y_1 = \sqrt x_2 \cos x_1$ and $y_2 = \sqrt x_2 \sin x_1$ where $x_1 \sim U[0, 2\pi]$ and $x_1 \sim exp(1/2)$. $x_1$ can be generated using `runif(0, 2*pi)`. For $x_2$ we can reuse the function `sample_exponential` we created in Part 1 to simulate from the exponential distribution with rate parameter $\lambda = 1/2$. Since all samples are iid, this also scales to $n$ samples. It should be noted that is is way faster to do this in a vectorized fashion. Since it only works for even numbers, we have to remove the last sample we generated if $n$ is odd.

```{R, echo=T}
  # TODO: only works with even n
  sample_standard_normal = function(n) {
    x_1 = runif(n/2, 0, 2*pi)
    x_2 = sample_exponential(n/2, lambda=1/2)
    y_1 = sqrt(x_2) * cos(x_1)
    y_2 = sqrt(x_2) * sin(x_1)
    return(c(y_1, y_2))
  }
```

We then evaluate the validity in terms of moments by comparing theoretical and empirical variation and mean when drawing $n = 1\text{E+5}$ samples using the `sample_standard_normal` function as defined above:

```{R, echo=T}
  n = 100000
  y = sample_standard_normal(n)
  
  cat("Empirical mean:", mean(y), "\n")
  cat("Theoretical mean:", 0, '\n')
  cat("----\n")
  cat("Empirical variance:", var(y), "\n")
  cat("Theoretical variance:", 1, "\n")

```

As we see, the moments are matching well. As for the distribution of the samples versus the analytic distribution, this has been show below. As you see, The samples overlaps well with the analytic expression:

```{R}
  library(ggplot2)
  df = data.frame(theo=seq(0,max(x),length.out=n), x=y)
  ggplot(df, aes(x=y)) +
    geom_histogram(aes(y = ..density.., color = "Sampled"), binwidth=0.01) +
    stat_function(fun=dnorm,geom = "line",size=0.7,args=list(0,1),aes(color="Analytic")) +
    ggtitle("Standard normal distribution - Sampled vs Analytic") +
    labs(x='x', y='density')
```

**TODO**: Here we could add a QQ-plot as well.

## Part 4 - d-variate normal distribution

Given $x \sim N_d(0, 1)$, we know that by linear transformation, 

$$
  \pmb{y} = A\pmb{x} + \pmb{\mu} \sim N_d(\pmb{\mu}, A A^T)
$$

So in order to sample from the $d$-variate normal distribution $y = N_d(\mu, \Sigma)$, we can first sample from $N_d(0, 1)$ using the function `sample_standard_normal`, and get $\pmb{y}$ by using the linear transformation proposed above. The matrix $A$ can be found by using Cholesky decomposition on $\Sigma$. This has been implemented in `sample_normal_d` as defined below.

```{R}
sample_normal_d = function(d, mu, sigma) {
  x = sample_standard_normal(d)
  y <- mu + t(chol(sigma))%*%x
  return(y)
}
```

We then generate $n = 1\text{E+5}$ samples using the $\pmb{\mu}$ and $\Sigma$ as defined below. 

$$
  \mu = \begin{pmatrix} 1.5 \\ 3 \end{pmatrix} , \quad \Sigma = \begin{pmatrix} 2 & 5 \\ 8 & 10 \end{pmatrix}
$$
```{R}
  
```