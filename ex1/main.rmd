
--- 
title: 'TMA4300 Computer Intensive Statistical Methods Exercise 1, Spring 2021'
output:
  pdf_document:
    toc: no
    toc_depth: '2'
  html_document:
    toc: yes
    toc_depth: 2
    toc_float: yes
date: "`r format(Sys.time(), '%d.%m.%Y')`"
subtitle: 'Group members: Eide, Jonathan and Lima-Eriksen, Leik'
---

# Problem A

## Part 1 - the exponential distribution

The exponential distribution with rate parameter $\lambda$ has CDF $F(x) = 1 - e^{-\lambda x}$. Sampling from this distribution is equivalent to sampling uniformly on the inverse CDF, i.e:

$$
\begin{aligned}
  u \sim U[0,1] \\
  f(x) = F^{-1}(u) = -\frac{1}{\lambda}\log{u}
\end{aligned}
$$
This is implemented in R the following way:
```{R, echo = T}
  sample_exponential = function(n, lambda) {
    uniforms = runif(n)
    return(exponentials = -1/lambda * log(uniforms))
  }
```

We then want to make sure that our function works correctly. We start off by checking that the sample mean matches the expected value $E[x] = \frac{1}{\lambda}$, and that the sample variance matches the variance $Var[x] = \frac{1}{\lambda^2}$:

```{R, echo = T}
  n = 100000
  lambda = 3.2
  
  x = sample_exponential(n, lambda)
  cat('Theoretical mean:', 1/lambda, '\n')
  cat('Sample mean:', mean(x), '\n----\n')
  cat('Theoretical variance:', 1/lambda^2, '\n')
  cat('Sample variance:', var(x), '\n')
```

We see that there is a good match between the theoretical and obtained values, which indicates that the function works as expected. Of course they would not be exactly equal because we only obtain a finite number of samples.

We then proceed to verify that the samples ensemble the same distribution as the exponential distribution. A good way to visualize this would be to plot a histogram of the samples, and then superimpose the analytic function $f(x) = \lambda e^{-\lambda x}$. This is shown in the figure below:

```{R exp_pdf}
  library(ggplot2)
  df = data.frame(theo=seq(0,max(x),length.out=n), x=x)
  ggplot(df, aes(x=x)) +
    geom_histogram(aes(y = ..density.., color = "Sampled"), binwidth=0.01) +
    stat_function(fun=dexp,geom = "line",size=0.7,args=(mean=lambda),aes(color="Analytical")) +
    ggtitle("Exponential pdf - Sampled vs Analytical") +
    labs(x='x', y='density') +
    xlim(0, 1.5)
```

From \@ref{exp_pdf} we see that the samples follow the exponential distribution perfectly. We thereby conclude that the function samples as expected.

## Part 2

We then want to create a function for sampling from the pdf $g(x)$ as shown below. 

$$
g(x) = \begin{cases} 
  c x^{\alpha - 1}, \quad 0 < x < 1, \\
  c e^{-x}, \quad 1 \leq x, \\
  0, \quad otherwise
\end{cases}
$$

First, note that the normalizing constant $c$ can be found by using the property $\int_{-\infty}^{\infty} g(x) dx = 1$:
$$
  \begin{aligned}
    1 &= \int_0^1 c x^{\alpha - 1} dx + \int_1^{\infty} c e^{-x} dx \\
    &= \frac{c}{\alpha} + \frac{c}{e} \\
    &\Rightarrow c  = \frac{\alpha e}{\alpha + e}
  \end{aligned}
$$
Our aim is then to find an analytic expression for the inverse CDF, so that we can sample uniformly from it to generate samples from $g(x)$ in the same way as we did in Part 1. We start off by first finding the CDF $G(X) = P(X \leq x) = \int_{-\infty}^{x} g(x) dx$:

$$
  \begin{aligned}
  G(x) &= \begin{cases}
    \int_0^xc x^{\alpha - 1} dx ,  \quad x \in (0, 1) \\
    \int_0^x c x^{\alpha - 1}dx + \int_1^xc e^{-x} dx,  \quad x \in [1, \infty)
  \end{cases}\\
  &= \begin{cases}
    \frac{c}{\alpha} x^\alpha,  \quad x \in (0, 1) \\
    \frac{c}{\alpha} +c (e^{-1} - e^{-x}),  \quad x \in [1, \infty)
  \end{cases}
  \end{aligned}
$$

Then the inverse CDF, $G^{-1}(x)$ is found by solving $G^{-1}(G(x)) = x$. The definition limits for the inverse function is obviously different, and is found by evaluating $G^{-1}(x) = 1$, since this is the $x$-value for which the analytic expression changes for the CDF.

$$
  G^{-1}(x) = \begin{cases}
    (\frac{\alpha}{c} x)^{1/\alpha}, \quad x \in (0, \frac{c}{\alpha}), \\
    \ln{\frac{c}{1 - x}} \quad \forall \quad x \in [\frac{c}{\alpha}, 1), \\
    0, \text{ otherwise.}
  \end{cases}
$$


To sample from $g(x)$ is then equivalent to generate $u \sim U[0,1]$, and evaluate $G^{-1}(u)$. Below the `density_g` function allows us to evaluate the density analytically for given values of $x$. Furthermore, `sample_g` is used to sample from $g$ using the inverse CDF as previously specified.

```{R, echo = T, eval = T}
  density_g = function(x, alpha) {
    c = alpha*exp(1)/(alpha + exp(1))
    density = vector(length = length(x))
    density[x < 1.] = c*x[x<1.]^(alpha-1)
    density[x >= 1.] = c*exp(-x[x>=1.])
    return(as.double(density))
  }


  sample_g = function(n, alpha) {
    c = (alpha * exp(1)) / (alpha + exp(1))
    u = runif(n)
    samples = vector(length=length(u))
    samples[u < c/alpha] = (alpha/c*u[u < c/alpha])^(1/alpha)
    samples[u >= c/alpha] = log(c / (1 - u[u >= c/alpha]))
    return(samples)
  }
```

We then want to compare the expected value and variance with the empirical mean and variance respectively. The moments can be calculated as follows:

$$
  E[X] = \int_0^\infty x g(x) dx = \frac{c}{\alpha + 1} + 2 \frac{c}{e}
$$

$$
  Var[X] = E[X^2] -E[X]^2 = \int_0^\infty x^2g(x)dx - E[X]^2 = \frac{c}{\alpha + 2} + 5\frac{c}{e} - (\frac{c}{\alpha + 1} + 2 \frac{c}{e})^2
$$
For $n=1\text{E+5}$ samples with $\alpha = 0.7$, the empirical and theoretical mean and variances are respectively:

```{R, echo = T, eval = T}
  n=100000
  alpha = 0.7
  x = sample_g(n, alpha)
  cat("Empirical mean:", mean(x), "\n")
  c = (alpha * exp(1)) / (alpha + exp(1))
  cat("Theoretical mean:", c/(alpha+1) + 2 * c / exp(1), "\n")
  cat("----\n")
  cat("Empirical variance:", var(x), "\n")
  cat("Theoretical variance:", c/(alpha+2) + 5*c/exp(1) - (c/(alpha+1) + 2*c/exp(1))^2)
```

Lastly, we create a histogram of the samples with the analytic density superimposed. It seems that they coincides well. 
```{R}
  library(ggplot2)
  df <- data.frame(theo=seq(0,max(x),length.out=n), x=x)
  ggplot(df, aes(x = x)) +
    geom_histogram(aes(y = ..density.., color = "Sampled"), binwidth=0.01) +
    stat_function(fun=density_g,geom = "line",size=0.5,args=(mean=alpha),aes(color="Analytical")) + 
    xlim(0, 2) + 
    ggtitle("Piecewise defined pdf - Sampled vs Analytical") + labs(x='x', y='density')
```

## Part 3 - The Box-Muller Algorithm

In order to generate $n$ independent samples from the standard normal distribution, we can use the Box-Muller algorithm. The algorithm makes it possible to generate two iid. samples from $N(0,1)$ according to $y_1 = \sqrt x_2 \cos x_1$ and $y_2 = \sqrt x_2 \sin x_1$ where $x_1 \sim U[0, 2\pi]$ and $x_1 \sim exp(1/2)$. $x_1$ can be generated using `runif(0, 2*pi)`. For $x_2$ we can reuse the function `sample_exponential` we created in Part 1 to simulate from the exponential distribution with rate parameter $\lambda = 1/2$. Since all samples are iid, this also scales to $n$ samples. It should be noted that is is way faster to do this in a vectorized fashion. Since it only works for even numbers, we have to remove the last sample we generated if $n$ is odd.

```{R, echo=T}
  # TODO: only works with even n
  sample_standard_normal = function(n) {
    x_1 = runif(n/2, 0, 2*pi)
    x_2 = sample_exponential(n/2, lambda=1/2)
    y_1 = sqrt(x_2) * cos(x_1)
    y_2 = sqrt(x_2) * sin(x_1)
    return(c(y_1, y_2))
  }
```

We then evaluate the validity in terms of moments by comparing theoretical and empirical variation and mean when drawing $n = 1\text{E+5}$ samples using the `sample_standard_normal` function as defined above:

```{R, echo=T}
  n = 100000
  y = sample_standard_normal(n)
  
  cat("Empirical mean:", mean(y), "\n")
  cat("Theoretical mean:", 0, '\n')
  cat("----\n")
  cat("Empirical variance:", var(y), "\n")
  cat("Theoretical variance:", 1, "\n")

```

As we see, the moments are matching well. As for the distribution of the samples versus the analytic distribution, this has been show below. As you see, The samples overlaps well with the analytic expression:

```{R}
  library(ggplot2)
  df = data.frame(theo=seq(0,max(x),length.out=n), x=y)
  ggplot(df, aes(x=y)) +
    geom_histogram(aes(y = ..density.., color = "Sampled"), binwidth=0.01) +
    stat_function(fun=dnorm,geom = "line",size=0.7,args=list(0,1),aes(color="Analytic")) +
    ggtitle("Standard normal distribution - Sampled vs Analytic") +
    labs(x='x', y='density')
```

**TODO**: Here we could add a QQ-plot as well.

## Part 4 - d-variate normal distribution

Given $x \sim N_d(0, 1)$, we know that by linear transformation, 

$$
  \pmb{y} = A\pmb{x} + \pmb{\mu} \sim N_d(\pmb{\mu}, A A^T)
$$

So in order to sample from the $d$-variate normal distribution $y = N_d(\mu, \Sigma)$, we can first sample from $N_d(0, 1)$ using the function `sample_standard_normal`, and get $\pmb{y}$ by using the linear transformation proposed above. The matrix $A$ can be found by using Cholesky decomposition on $\Sigma$. This has been implemented in `sample_normal_d` as defined below.

```{R}
sample_normal_d = function(d, mu, sigma) {
  x = sample_standard_normal(d)
  y <- mu + t(chol(sigma))%*%x
  return(y)
}
```

We then generate $n = 1\text{E+5}$ samples using the $\pmb{\mu}$ and $\Sigma$ as defined below. 

$$
  \mu = \begin{pmatrix} 1.5 \\ 3 \end{pmatrix} , \quad \Sigma = \begin{pmatrix} 5 & 7 \\ 7 & 10 \end{pmatrix}
$$
```{R}
d = 2
n = 100000
mu = c(1.5, 3)
sigma = matrix(c(5,7,7,10),ncol=d,nrow=d)
samples = matrix(NA, ncol=d, nrow=n)
for (i in 1:n){
  samples[i,] = sample_normal_d(d,mu,sigma) # Draw n samples
}

average = apply(samples, 2, mean)
print("Mean vector:")
print.data.frame(data.frame(average))
covmat = cov(samples)
print("Covariance matrix:")
print.data.frame(data.frame(covmat))
```

# Part B

## Problem 1

We want to find a way to sample from a gamma distribution $f(x)$ with parameters $\alpha \in (0, 1)$ and $\beta = 1$ as defined below. In order to do this, we can use rejection sampling with $g(x)$ as the proposal distribution. This is possible because the support of $g(x)$ includes the support of $f(x)$; in other words, $g(x) > 0$ whenever $f(x) > 0$. As a consequence of this, there exists a $c > 1$ such that $\left\{\frac{f(x)}{g(x)} \leq c, x : f(x) > 0\right\}$. 

$$
  f(x) = \begin{cases}
    \frac{1}{\Gamma(\alpha)} x^{\alpha-1} e^{-x}, \quad &0<x \\
    0, &\text{otherwise.}
  \end{cases}
$$

Rejection sampling is done in two steps. First, we generate $x \sim g(x)$. We then compute $\alpha = \frac{1}{c} \cdot  \frac{f(x)}{g(x)}$, and generate $u \sim U[0,1]$. If $u \leq \alpha$, then $x$ is a sample from $f(x)$, and if not we have to redo the whole process until it is. The overall acceptance probability $P_a$ is found to be the following:

$$
  P_a = P\left(U \leq \frac{1}{c} \cdot  \frac{f(x)}{g(x)}\right) = \int_{-\infty}^\infty \frac{f(x)}{c \cdot g(x)} g(x) dx = c^{-1}\int_{-\infty}^\infty f(x) dx= c^{-1}
$$

So in order to maximize the acceptance probability, we have to minimize $c$. Since we have to find a $c$ which is greater than $\frac{f(x)}{g(x)} \forall x \in \mathbb{R}$, the lowest possible $c$ would be the highest value the ratio $\frac{f(x)}{g(x)}$ can attain, i.e.
$$
  \begin{align}
    c &= \sup_x \frac{f(x)}{g(x)} \\
      &= \sup_x \begin{cases}
      \frac{e^{-x}}{\Gamma(\alpha)} \left(\frac{1}{\alpha} + \frac{1}{e}\right), \quad &0 < x < 1, \\
      \frac{x^{\alpha-1}}{\Gamma(\alpha)}\left(\frac{1}{\alpha} + \frac{1}{e}\right), \quad &1 \leq x
    \end{cases} \\
    &= \frac{\alpha^{-1} + e^{-1}}{\Gamma(\alpha)}
  \end{align}
$$

This means that the lowest acceptance probability we can achieve is $P_\text{a,lowest} = \frac{\Gamma(\alpha)}{\alpha^{-1} + e^{-1}}$. A sampling algorithm which uses this $c$ has been implemented below in `R`. Here we exploit the fact that it on average takes $c$ iterations to generate 1 sample from the target distribution $f(x)$. Then the actual number of samples we need in order to sample $n$ times from $f(x)$ should be pretty close to $n c$ when $n$ is large. This speeds up the execution significantly, as execution time is of concern only for large $n$. After sampling once, we resample with $c$ times the number of samples we are missing. This should then converge pretty fast.

```{R}
  density_f = function(x, alpha) {
    return(dgamma(x, alpha, rate=1))
  }
  
  sample_f = function(n, alpha) {
    c = (1/alpha + exp(-1)) / gamma(alpha)
    samples = vector()
    repeat {
      num_missing_samples = n - length(samples)
      
      x = sample_g(as.integer(num_missing_samples*c), alpha)
      
      acceptance_probs = density_f(x, alpha) / (c * density_g(x, alpha))
      u = runif(as.integer(num_missing_samples*c))
      samples = c(samples, x[u <= acceptance_probs])
      if(length(samples) >= n) {
        break
      }
    }
    # Do not return excess samples:
    return(samples[0:n])
  }
```
We then want to evaluate the correctness of our sample in terms of expectation value and variance. Below we have calculated the sample mean and variance, and compared them with their respective theoretical values. We see that they match well.
```{R}
  n = 100000
  alpha = 0.6
  x = sample_f(n, alpha)
  cat("Theoretical mean:", alpha, "\n")
  cat("Sample mean:", mean(x), "\n")
  cat("----", "\n")
  
  cat("Theoretical variance:", alpha, "\n")
  cat("Sample variance:", var(x), "\n")
```

Furthermore, it is important to check that a histogram of the samples overlaps well with the analytic expression for $f(x)$. Such a plot has been created below. We see a good ovelap, which further strengthens our belief that the sampler works as intended.

```{R}
  n = 100000
  alpha = 0.6
  x = sample_f(n, alpha)
  df = data.frame(x)
  ggplot(df, aes(x = x)) +
    geom_histogram(aes(y = ..density.., color = "Sampled"), binwidth=0.01) +
    stat_function(fun=dgamma,geom = "line",size=0.5,args=(alpha=alpha),aes(color="Analytical")) +
    xlim(0, 4) + ylim(0, 3) + 
    ggtitle("Gamma distribution using rejection sampling") + labs(x='x', y='density')
```
